experiment_name: vlm_training
max_epochs: 100
early_stopping: false
save_every_n_epochs: 200
save_every_n_train_steps: null
patience: 5
log_every_n_steps: 5
val_check_interval: 0.5
gradient_clip_val: 1.0
accumulate_grad_batches: 1
# choose from [64, 32, 16, 'transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']
precision: 16-mixed
# devices can be int or str, but currently don't support list since OmegaConf doesn't support Union[list]
devices: auto
resume_from_checkpoint: false
checkpoint_path: null
num_training_samples: 10
batch_size: 128

defaults:
  - unfreeze: pretrain
  - learning_rate: default
  - weight_decay: default
  - optimizer: adamw
  - scheduler: default
