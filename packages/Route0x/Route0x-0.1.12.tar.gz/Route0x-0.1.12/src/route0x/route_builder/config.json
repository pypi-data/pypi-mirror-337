{
    "model_name": "sentence-transformers/all-mpnet-base-v2",
    "epochs": 1,
    "max_steps": 100,
    "batch_size": 16,
    "lr": 2e-5,
    "loss_funct_name": "PairwiseArcFaceFocalLoss",
    "warmup_proportion": 0.05,
    "min_samples": 12,
    "fallback_samples_per_route": 30,
    "test_samples_per_route": 10,
    "label_column": "label",
    "text_column": "text",
    "route_template": "{}",
    "llm_name": "llama3.1",
    "enable_id_oos_gen": true,
    "enable_synth_data_gen": true,
    "enable_test_dataset_gen": false,
    "add_typo_robustness": false,
    "max_query_len": 24,
    "instruct_llm": "",
    "output_dir": "output",
    "local_llm_host": "http://localhost:11434",
    "hosted_llm_families": { "openai": ["gpt"], "anthropic": ["claude"], "google": ["gemini"]},
    "seed": 1234,
    "oos_label": "NO_NODES_DETECTED",
    "expected_oos_proportion": 0.1,
    "nn_for_oos_detection": 10,
    "invalid_routes": ["gibberish", "mojibake", "chitchat", "non-english", "profanity"],
    "add_additional_invalid_routes": false
}
