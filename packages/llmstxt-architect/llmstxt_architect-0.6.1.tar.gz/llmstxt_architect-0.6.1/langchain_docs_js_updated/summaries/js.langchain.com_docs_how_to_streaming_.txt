[How to: stream runnables](https://js.langchain.com/docs/how_to/streaming/): LLM should read this page when (implementing streaming functionality with LangChain JS, troubleshooting streaming issues in LLM applications, or learning how to display partial outputs while an LLM generates responses) (Comprehensive guide on streaming in LangChain JS, covering two main approaches: .stream() for final output streaming and streamEvents() for intermediate steps streaming. Includes code examples for working with Chat Models, handling JSON outputs, filtering events, handling non-streaming components, and implementing streaming over HTTP connections.)

