{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core components\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_manage_memory_tool, create_search_memory_tool\n",
    "\n",
    "# Set up storage\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create an agent with memory capabilities\n",
    "agent = create_react_agent(\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    tools=[\n",
    "        # Memory tools use LangGraph's BaseStore for persistence (4)\n",
    "        create_manage_memory_tool(namespace=(\"memories\",)),\n",
    "        create_search_memory_tool(namespace=(\"memories\",)),\n",
    "    ],\n",
    "    store=store,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a new memory\n",
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Remember that I prefer dark mode.\"}]})\n",
    "\n",
    "# Retrieve the stored memory\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What are my lighting preferences?\"}]})\n",
    "print(response[\"messages\"][-1].content)\n",
    "# Output: \"You've told me that you prefer dark mode.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.func import entrypoint\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langmem import create_memory_store_manager\n",
    "\n",
    "store = InMemoryStore(  #\n",
    "    index={\n",
    "        \"dims\": 1536,\n",
    "        \"embed\": \"openai:text-embedding-3-small\",\n",
    "    }\n",
    ")\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "# Create memory manager Runnable to extract memories from conversations\n",
    "memory_manager = create_memory_store_manager(\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    # Store memories in the \"memories\" namespace (aka directory)\n",
    "    namespace=(\"memories\",),  #\n",
    ")\n",
    "\n",
    "\n",
    "@entrypoint(store=store)  # Create a LangGraph workflow\n",
    "async def chat(message: str):\n",
    "    print(\"Calling llm\")\n",
    "    response = llm.invoke(message)\n",
    "    print(\"LLM response: \", response)\n",
    "\n",
    "    start = time.time()\n",
    "    # memory_manager extracts memories from conversation history\n",
    "    # We'll provide it in OpenAI's message format\n",
    "    to_process = {\"messages\": [{\"role\": \"user\", \"content\": message}] + [response]}\n",
    "\n",
    "    print(\"Calling memory manager\")\n",
    "    await memory_manager.ainvoke(to_process)  #\n",
    "    end = time.time()\n",
    "    print(f\"Time taken: {end - start} seconds\")\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run conversation as normal\n",
    "response = await chat.ainvoke(\n",
    "    \"I like dogs. My dog's name is Fido.\",\n",
    ")\n",
    "\n",
    "print(response)\n",
    "# Output: That's nice! Dogs make wonderful companions. Fido is a classic dog name. What kind of dog is Fido?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store.search((\"memories\",)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi, my name is bob', additional_kwargs={}, response_metadata={}, id='c900d1a8-12e2-43f4-a50e-08997fb477d6'),\n",
       "  AIMessage(content='Hello Bob! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 13, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-d36879da-fc8c-44aa-af51-33b286a1d2dd-0', usage_metadata={'input_tokens': 13, 'output_tokens': 11, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='write a short poem about cats', additional_kwargs={}, response_metadata={}, id='f8c0cccc-970c-4616-b861-3fd3656b41c7'),\n",
       "  AIMessage(content='In sunlit naps, they softly purr,  \\nSilent shadows in a furred blur.  \\nWhiskers twitch as dreams unfold,  \\nIn moonlit prowls, tales are told.  \\n\\nTiny tigers, fierce and free,  \\nRulers of homes, yet wild at sea.  \\nWith grace, they leap, with stealth, they tread,  \\nNine lives lived, and dreams widespread.  \\n\\nIn their eyes, a world unknown,  \\nA mystic realm all their own.  \\nCats, the poets of the night,  \\nWith gentle paws, they write their flight.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 119, 'prompt_tokens': 37, 'total_tokens': 156, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-89c769b1-5a6b-4563-99df-cf0eddf8356b-0', usage_metadata={'input_tokens': 37, 'output_tokens': 119, 'total_tokens': 156, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content='now do the same but for dogs', additional_kwargs={}, response_metadata={}, id='e588de42-09c7-4e48-8597-0564f330ac5d'),\n",
       "  AIMessage(content=\"In fields they bound with joyous glee,  \\nFaithful hearts, wild and free.  \\nGolden eyes that speak of love,  \\nIn loyal grins, they rise above.  \\n\\nPaws that dance on trails and street,  \\nUnwavering friends, life's journey sweet.  \\nWith wagging tails, they share delight,  \\nGuardians by day, companions by night.  \\n\\nBy nature, brave, with spirits bright,  \\nIn their presence, dark turns to light.  \\nDogs, the bonds that warm our days,  \\nWith every bark, our spirits they raise.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 170, 'total_tokens': 287, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6ec83003ad', 'finish_reason': 'stop', 'logprobs': None}, id='run-561ff67a-f86c-4539-8df8-05ff380f81c0-0', usage_metadata={'input_tokens': 170, 'output_tokens': 117, 'total_tokens': 287, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  HumanMessage(content=\"what's my name?\", additional_kwargs={}, response_metadata={}, id='c54c8f6a-e8ac-4fd7-b4cf-3c47fc83342f'),\n",
       "  AIMessage(content='Your name is Bob.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 179, 'total_tokens': 185, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_83df987f64', 'finish_reason': 'stop', 'logprobs': None}, id='run-8839d708-fc35-4868-bb61-ddb10ddd73c7-0', usage_metadata={'input_tokens': 179, 'output_tokens': 6, 'total_tokens': 185, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'summary': RunningSummary(summary='Bob introduced himself, and I greeted him, offering assistance. Bob then requested a short poem about cats, which I provided.', summarized_message_ids={'run-89c769b1-5a6b-4563-99df-cf0eddf8356b-0', 'c900d1a8-12e2-43f4-a50e-08997fb477d6', 'f8c0cccc-970c-4616-b861-3fd3656b41c7', 'run-d36879da-fc8c-44aa-af51-33b286a1d2dd-0'})}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langmem.short_term import RunningSummary, summarize_messages\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "# NOTE: we're also setting max output tokens for the summary\n",
    "# this should match max_summary_tokens in `summarize_messages` for better\n",
    "# token budget estimates\n",
    "summarization_model = model.bind(max_tokens=128)\n",
    "\n",
    "\n",
    "# We will keep track of our running summary in the graph state\n",
    "class SummaryState(MessagesState):\n",
    "    summary: RunningSummary | None\n",
    "\n",
    "\n",
    "# Define the node that will be calling the LLM\n",
    "def call_model(state: SummaryState) -> SummaryState:\n",
    "    # We will attempt to summarize messages before the LLM is called\n",
    "    # If the messages in state[\"messages\"] fit into max tokens budget,\n",
    "    # we will simply return those messages. Otherwise, we will summarize\n",
    "    # and return [summary_message] + remaining_messages\n",
    "    summarization_result = summarize_messages(\n",
    "        state[\"messages\"],\n",
    "        # IMPORTANT: Pass running summary, if any. This is what\n",
    "        # allows summarize_messages to avoid re-summarizing the same\n",
    "        # messages on every conversation turn\n",
    "        running_summary=state.get(\"summary\"),\n",
    "        # by default this is using approximate token counting,\n",
    "        # but you can also use LLM-specific one, like below\n",
    "        token_counter=model.get_num_tokens_from_messages,\n",
    "        model=summarization_model,\n",
    "        max_tokens=256,\n",
    "        max_summary_tokens=128,\n",
    "    )\n",
    "    response = model.invoke(summarization_result.messages)\n",
    "    state_update = {\"messages\": [response]}\n",
    "    # If we generated a summary, add it as a state update and overwrite\n",
    "    # the previously generated summary, if any\n",
    "    if summarization_result.running_summary:\n",
    "        state_update[\"summary\"] = summarization_result.running_summary\n",
    "    return state_update\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "builder = StateGraph(SummaryState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "# It's important to compile the graph with a checkpointer,\n",
    "# otherwise the graph won't remember previous conversation turns\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Invoke the graph\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "graph.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "graph.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "graph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "graph.invoke({\"messages\": \"what's my name?\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
