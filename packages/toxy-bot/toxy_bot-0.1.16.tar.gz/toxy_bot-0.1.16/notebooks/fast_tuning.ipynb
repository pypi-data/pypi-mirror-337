{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Hugging Face BERT with PyTorch Lightning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the following cells will train the model using settings that are shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "# from lightning.pytorch.profilers import PyTorchProfiler\n",
    "\n",
    "from toxy_bot.ml.datamodule import AutoTokenizerDataModule\n",
    "from toxy_bot.ml.module import SequenceClassificationModule\n",
    "from toxy_bot.ml.utils import create_dirs\n",
    "from toxy_bot.ml.config import Config, DataModuleConfig, ModuleConfig, TrainerConfig\n",
    "\n",
    "from toxy_bot.ml.trainer import train\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's configure some basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: google/bert_uncased_L-4_H-512_A-8\n",
      "Learning rate: 3e-05\n",
      "Dataset: anitamaxvim/jigsaw-toxic-comments\n",
      "Batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# model and dataset\n",
    "model_name = ModuleConfig.model_name\n",
    "lr = ModuleConfig.learning_rate\n",
    "dataset_name = DataModuleConfig.dataset_name\n",
    "batch_size = DataModuleConfig.batch_size\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Learning rate: {lr}\")\n",
    "print(f\"Dataset: {dataset_name}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "\n",
    "# paths\n",
    "cache_dir = Config.cache_dir\n",
    "log_dir = Config.log_dir\n",
    "ckpt_dir = Config.ckpt_dir\n",
    "# prof_dir = Config.prof_dir\n",
    "perf_dir = Config.perf_dir\n",
    "# creates dirs to avoid failure if empty dir has been deleted\n",
    "create_dirs([cache_dir, log_dir, ckpt_dir, perf_dir])\n",
    "\n",
    "# set matmul precision\n",
    "# see https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-4_H-512_A-8 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/dbozbay/Dev/toxy-bot/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "[2025-03-27 16:28:36.163277] Data cache exists. Loading from cache.\n",
      "Parameter 'function'=<bound method AutoTokenizerDataModule.preprocess of <toxy_bot.ml.datamodule.AutoTokenizerDataModule object at 0x122b11ad0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|██████████| 135635/135635 [01:24<00:00, 1601.82 examples/s]\n",
      "Map: 100%|██████████| 23936/23936 [00:14<00:00, 1664.67 examples/s]\n",
      "\n",
      "  | Name      | Type                          | Params | Mode \n",
      "--------------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 28.8 M | eval \n",
      "1 | accuracy  | MultilabelAccuracy            | 0      | train\n",
      "2 | f1_score  | MultilabelF1Score             | 0      | train\n",
      "3 | precision | MultilabelPrecision           | 0      | train\n",
      "4 | recall    | MultilabelRecall              | 0      | train\n",
      "--------------------------------------------------------------------\n",
      "28.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.8 M    Total params\n",
      "115.067   Total estimated model params size (MB)\n",
      "4         Modules in train mode\n",
      "87        Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbozbay/Dev/toxy-bot/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbozbay/Dev/toxy-bot/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 3/8478 [09:01<425:00:35,  0.01it/s, v_num=1, train_loss=0.678]"
     ]
    }
   ],
   "source": [
    "train(perf=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
