[Evaluate a RAG application | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/evaluation/tutorials/rag): LLM should read this page when evaluating RAG applications, implementing metrics for RAG evaluation, or designing comprehensive evaluation frameworks for retrieval-based systems. This tutorial demonstrates how to evaluate RAG applications using LangSmith, covering dataset creation, running evaluations, and implementing four key evaluation metrics: correctness (answer vs reference), relevance (response vs input), groundedness (response vs retrieved docs), and retrieval relevance (retrieved docs vs input).

