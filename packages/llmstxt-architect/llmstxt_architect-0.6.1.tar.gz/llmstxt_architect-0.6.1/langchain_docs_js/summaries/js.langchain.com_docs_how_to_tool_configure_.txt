[](https://js.langchain.com/docs/how_to/tool_configure/): LLM should read this page when: 1) Developing a custom tool in LangChain and needing to access the RunnableConfig 2) Wanting to invoke other runnables like chat models or retrievers from within a custom tool 'This page explains how to access the RunnableConfig object from within a custom LangChain tool, which allows properly configuring and tracing sub-invocations of other runnables like chat models or retrievers.'

