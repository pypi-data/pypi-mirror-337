[Prompt engineering how-to guides | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides): LLM should read this page when looking to create, update, or manage prompts for LLMs, when working with LangSmith's prompt playground or canvas, and when using few-shot prompting. This page provides how-to guides on creating and updating prompts, managing prompts programmatically, using the prompt playground and canvas, running the playground against custom model servers, testing prompts over datasets, using few-shot example selection, and leveraging the LangChain Hub.

[Create a prompt | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/create_a_prompt): LLM should read this page when creating a new prompt, saving a prompt, or adding metadata to a prompt. This page provides instructions on how to create a new prompt using the LangSmith UI, save the prompt as private or public, and add metadata like descriptions and use cases.

[Run the playground against a custom LangServe model server | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_endpoint): LLM should read this page when considering how to run the LangSmith playground against a custom model server, when adding configurable fields to a custom model, and when using a custom model in the LangSmith playground. This page provides guidance on deploying a custom model server using LangServe, adding configurable fields to the model, and using the custom model in the LangSmith playground.

[Run the playground against an OpenAI-compliant model provider/proxy | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/custom_openai_compliant_model): LLM should read this page when it needs to use a model that is compatible with the OpenAI API in the LangSmith playground, or when it needs to deploy and use a custom model in the LangSmith playground. This page explains how to deploy an OpenAI-compliant model provider/proxy, and how to use that model in the LangSmith playground by setting the OpenAI Proxy Provider option.

[LangChain Hub | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/langchain_hub): LLM should read this page when needing an overview of the LangChain Hub, wanting to search for existing prompts, or forking prompts to a personal organization. The page covers navigating the LangChain Hub section, which lists all publicly available prompts that can be searched, viewed in detail, run in the playground, and programmatically accessed via the SDK.

[Manage prompts programmatically | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically): LLM should read this page when managing prompts programmatically, pushing/pulling prompts to/from the LangChain Hub, or converting prompts to work with the OpenAI or Anthropic APIs directly. Provides instructions and code examples for installing packages, configuring environment variables, pushing and pulling prompts, using prompts without LangChain, and listing/deleting/liking prompts through the LangSmith SDK.

[Managing Prompt Settings | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/managing_model_configurations): LLM should read this page when managing prompt settings, creating/editing configurations, or setting default configurations. Covers managing model configurations, tool settings, prompt formatting, and extra parameters in the LangSmith playground.

[Open a prompt from a trace | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/open_a_prompt_from_a_trace): LLM should read this page when it needs to open a prompt from a trace for debugging or analysis, when it wants to link prompts used in code to the actual prompt definitions, when it needs to filter traces by prompt metadata This page explains how to find and open prompts from traces in the LangSmith UI, including showing the prompt link in traces, viewing prompt metadata, and filtering traces by prompt attributes

[Prompt Canvas | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_canvas): LLM should read this page when editing prompts, iterating on long prompts, or making stylistic/tonal changes to prompts The page describes the Prompt Canvas feature in LangSmith, which allows editing prompts with LLM assistance, quick actions to modify prompts, custom quick actions, diffing versions, and saving/using updated prompts.

[Prompt Tags | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/prompt_tags): LLM should read this page when trying to understand prompt tags in LangSmith, when managing different versions of prompts, when deploying prompts to different environments. This page covers an overview of prompt tags, how to create/move/delete tags, using tags in code, and common use cases like environment-specific tags and version control.

[Testing over a dataset | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/testing_over_dataset): LLM should read this page when testing prompts over a dataset, evaluating prompt performance, or running experiments with evaluators. This page explains how to test prompts over a dataset in the LangSmith Playground, select evaluators for the experiment, and run the experiment to evaluate prompt performance.

[Update a prompt | ğŸ¦œï¸ğŸ› ï¸ LangSmith](https://docs.smith.langchain.com/prompt_engineering/how_to_guides/update_a_prompt): LLM should read this page when updating a prompt's description, use cases or content, or versioning a prompt. Covers how to edit prompt metadata, update prompt content in the playground, commit changes as new versions, and view commit history.

