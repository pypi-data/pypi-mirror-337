[Concepts](https://langchain-ai.github.io/langgraph/concepts/): LLM should read this page when: 1. Understanding LangGraph workflows and graph primitives 2. Learning about multi-agent systems in LLM applications 3. Implementing human-in-the-loop features in a LangGraph application LangGraph provides explanations of key concepts, including LangGraph Glossary, agentic patterns, multi-agent systems, breakpoints, human-in-the-loop, time travel, persistence, memory, streaming, and functional API.

[Agent architectures](https://langchain-ai.github.io/langgraph/concepts/agentic_concepts/): Line 1: LLM should read this page when an LLM is trying to solve complex problems, requires tool access or multi-step decision making. Line 2: This page discusses different types of agent architectures for LLMs, including routers, tool calling agents like ReAct, and various features such as memory and planning.

[Application Structure](https://langchain-ai.github.io/langgraph/concepts/application_structure/): Line 1: LLM should read this page when deploying LangGraph applications using Python or JavaScript and need to understand the application structure. Line 2: The page provides a typical structure for a LangGraph application, including dependencies, graphs, environment variables, and configuration files, and explains how these elements work together.

[Assistants](https://langchain-ai.github.io/langgraph/concepts/assistants/): LLM should read this page when building complex agents with frequent configuration changes, modifying agent behavior through prompt and model adjustments, or managing assistant versions to track and compare different configurations. Assistants offer a way to make and save rapid changes to agent configuration, provide versioning capabilities for tracking changes over time, and can be used to configure agents in a no-code manner.

[Authentication & Access Control](https://langchain-ai.github.io/langgraph/concepts/auth/): Line 1: 'LLM should read this page when implementing LangGraph auth handlers for conversational data model' Line 2: 'This page provides a comprehensive overview of LangGraph authentication, including handler types and supported actions'

[Breakpoints](https://langchain-ai.github.io/langgraph/concepts/breakpoints/): Line 1: LLM should read this page when the user is setting up a graph with breakpoints, debugging complex graph execution, or using human-in-the-loop workflows. Line 2: This page explains how to set static and dynamic breakpoints, use persistence layer, and manage graph state during execution.

[Bring Your Own Cloud (BYOC)](https://langchain-ai.github.io/langgraph/concepts/bring_your_own_cloud/): LLM should read this page when the user wants to deploy LangGraph graphs and manage their data on AWS, is setting up a new BYOC environment with existing AWS credentials, or needs to understand the architecture and requirements for LangChain Cloud. This page explains the Bring Your Own Cloud (BYOC) deployment options using LangGraph Platform, including the control plane and data plane architectures, requirements, how it works, and notes on customer use cases.

[Deployment Options](https://langchain-ai.github.io/langgraph/concepts/deployment_options/): LLM should read this page when planning a LangGraph Platform deployment for an Enterprise plan, reviewing the Cloud SaaS option, and understanding Bring Your Own Cloud (BYOC) setup. Overview: The page outlines four main options for deploying with the LangGraph Platform, including Self-Hosted Lite, Self-Hosted Enterprise, Cloud SaaS, and Bring Your Own Cloud. Each deployment option has its own set of features, limitations, and requirements.

[Double Texting](https://langchain-ai.github.io/langgraph/concepts/double_texting/): LLM should read this page when:  - A user is attempting to interact with a graph that has not yet finished running. - An LLM needs to understand the different options for handling "double texting" in LangGraph Server. A direct summary of main topics:  Options include Reject, Enqueue, Interrupt and Rollback, each addressing double texting in different ways.

[Durable Execution](https://langchain-ai.github.io/langgraph/concepts/durable_execution/): Here is the summary:  Line 1: 'LLM should read this page when using LangGraph with human-in-the-loop, encountering interruptions or errors, and needing to resume workflows.' Line 2: 'This page explains durable execution in LangGraph, including its benefits, requirements, and guidelines for deterministic and idempotent workflows.'

[FAQ](https://langchain-ai.github.io/langgraph/concepts/faq/): LLM should read this page when building agentic applications, deploying LangGraph applications, and using LLMs with LangGraph. LangGraph is an open-source orchestration framework for complex agentic systems, providing a more expressive framework to handle companies' unique tasks without restricting users to a single black-box cognitive architecture.

[Functional API](https://langchain-ai.github.io/langgraph/concepts/functional_api/): Line 1: 'LLM should read this page when implementing human-in-the-loop workflows, adding thread-level or cross-thread persistence, or designing multi-agent networks.' Line 2: 'The Functional API provides a guide to building workflows, agents, and human-in-the-loop applications with features such as interrupt handling, command primitives, state management, and memory mechanisms.'

[Why LangGraph?](https://langchain-ai.github.io/langgraph/concepts/high_level/): Line 1: LLM should read this page when building applications with workflow scaffolding, planning autonomous agents, or deploying LangGraph-based projects. Line 2: This page explains the benefits of using LangGraph, including persistence layer for memory and human-in-the-loop capabilities, streaming support for events and tokens, and debugging and deployment tools.

[Human-in-the-loop](https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/): Line 1: 'LLM should read this page when designing concurrent interrupt-based workflows' Line 2: 'The page explains the importance of handling multiple interrupts within a node, avoiding dynamic changes to the node's structure between executions'

[LangGraph CLI](https://langchain-ai.github.io/langgraph/concepts/langgraph_cli/): Line 1: LLM should read this page when the user is installing LangGraph CLI, wants to start a development server, or needs to create a custom Docker image. Line 2: The LangGraph CLI provides core functionality for building and running the LangGraph API server locally, including build, dev, up, dockerfile commands with features like hot reloading, debugger support, in-memory state, and virtual environment management.

[Cloud SaaS](https://langchain-ai.github.io/langgraph/concepts/langgraph_cloud/): LLM should read this page when: - Understanding LangGraph's Cloud SaaS and its prerequisites for deployment - Configuring and managing LangGraph Server deployments in the cloud - Optimizing CPU, memory, and scaling resources for development and production environments Deployment details, revision management, persistence, autoscaling, asynchronous deployment, and data retention policies.

[LangGraph Platform](https://langchain-ai.github.io/langgraph/concepts/langgraph_platform/): Line 1: LLM should read this page when deploying LangGraph applications to production, handling bursty requests, and implementing human-in-the-loop support. Line 2: Overview includes components (LangGraph Server, Studio, CLI, Python/JS SDK, Remote Graph) supporting development, deployment, debugging, and monitoring of agentic applications.

[LangGraph Server](https://langchain-ai.github.io/langgraph/concepts/langgraph_server/): LLM should read this page when deploying a LangGraph Server for complex application use cases, understanding its key features, and integrating with other components. LangGraph Server provides an API for creating and managing agent-based applications, supporting streaming endpoints, background runs, task queue, horizontally scalable infrastructure, double texting support, optimized checkpointer, human-in-the-loop endpoints, memory store, cron jobs, webhooks, and monitoring.

[LangGraph Studio](https://langchain-ai.github.io/langgraph/concepts/langgraph_studio/): LLM should read this page when the user is attempting to develop an LLM application on LangGraph Platform, or has a local LangGraph application running in-memory. LangGraph Studio provides a specialized agent IDE for visualization, interaction, and debugging of complex agentic applications with features such as visual graph editing, testing, debugging, assistant creation, thread management, long-term memory viewing, and path map usage.

[LangGraph Glossary](https://langchain-ai.github.io/langgraph/concepts/low_level/): 'LLM should read this page when building multi-agent systems, optimizing subgraph performance and communication, and troubleshooting LangGraph compilation issues.' 'Summary: LangGraph provides features for creating subgraphs as compiled nodes, function-based nodes, and adding subgraphs to parent graphs with shared or transformed state.'

[Memory](https://langchain-ai.github.io/langgraph/concepts/memory/): Line 1: LLM should read this page when it needs to understand how memories work in AI agents, specifically when implementing reflection or meta-prompting. Line 2: The page explains the different types of memories (episodic, procedural) and how they are used in AI agents, as well as methods for creating new memories, including writing them on the hot path and in the background.

[Multi-agent Systems](https://langchain-ai.github.io/langgraph/concepts/multi_agent/): Line 1: 'LLM should read this page when designing multi-agent systems with tool-calling supervisors' Line 2: 'This page covers the basics of communicating between agents in LangGraph, including graph state vs tool calls, different state schemas, and shared message lists.'

[Persistence](https://langchain-ai.github.io/langgraph/concepts/persistence/): Line 1: LLM should read this page when integrating LangGraph models for conversational AI, managing graph states and checkpointing, and configuring indexing settings for semantic search. Line 2: The page provides an overview of LangGraph's checkpointing capabilities, including checkpointer libraries, interfaces, and serialization protocols, as well as their applications in human-in-the-loop workflows, memory management, time travel, and fault-tolerance.

[LangGraph Platform Plans](https://langchain-ai.github.io/langgraph/concepts/plans/): LLM should read this page when: - To learn about the different plans available for LangGraph Platform - When evaluating deployment options for agentic applications - To understand the features and pricing of the LangGraph Platform LangGraph Platform offers three commercial plans (Developer, Plus, and Enterprise) with varying levels of access to deployment options, APIs, and other features.

[LangGraph Platform Architecture](https://langchain-ai.github.io/langgraph/concepts/platform_architecture/): LLM should read this page when the LLM is asked about LangGraph Platform Architecture, Postgres persistence layer, and Redis data storage.   Postgres is used as a database to store checkpoints and server resources in LGP, while Redis stores ephemeral metadata for worker communication and is used to organize server-worker communication.

[LangGraph's Runtime (Pregel)](https://langchain-ai.github.io/langgraph/concepts/pregel/): LLM should read this page when it is training on a variety of tasks that involve creating nodes and channels in a Pregel application.   The page provides an overview of the two APIs provided by the langgraph library, including how to use the StateGraph and entrypoint decorators to create Pregel applications. The page also explains how to compile a graph and inspect the nodes and channels associated with it. LLM should read this page when it is trying to understand how to apply the StateGraph and entrypoint decorators in different scenarios.   The page provides examples of how to use the StateGraph decorator to create Pregel applications, including how to add nodes and edges to a graph. The page also explains how to use the entrypoint decorator to define functions that take input and return output. LLM should read this page when it is trying to troubleshoot issues with node creation or channel assignment in Pregel applications.   The page provides information on how to inspect the nodes and channels associated with a Pregel application, including how to use the compile method to access the graph. The page also explains how to use the checkpoint module to save and load Pregel applications. LLM should read this page when it is trying to optimize node creation or channel assignment in Pregel applications.   The page provides information on how to customize the behavior of the StateGraph decorator, including how to add custom nodes and edges. The page also explains how to use the checkpoint module to save and load Pregel applications.

[LangGraph Platform: Scalability & Resilience](https://langchain-ai.github.io/langgraph/concepts/scalability_and_resilience/): LLM should read this page when adding a large number of instances to a LangGraph Platform service, implementing load balancing, or understanding queue scalability. The page discusses server and queue scalability with stateless instances, resilience features including periodic heartbeats, shutdown modes, Postgres and Redis failover mechanisms, and retry policies for transient errors.

[LangGraph SDK](https://langchain-ai.github.io/langgraph/concepts/sdk/): LLM should read this page when the user is setting up LangGraph Platform, installing LangGraph SDK packages for Python or JS/TS, and seeking API reference documentation. LangGraph provides a Python and JS/TS SDK for interacting with the server API, offers synchronous and asynchronous client options, and includes additional resources such as CLI API Reference.

[Self-Hosted](https://langchain-ai.github.io/langgraph/concepts/self_hosted/): LLM should read this page when deploying LangGraph Platform, setting up a self-hosted deployment for testing purposes, and understanding the requirements for running LangGraph Server. The main topics include Self-Hosted Lite and Enterprise versions, deployment options, Helm chart, and setup instructions.

[Streaming](https://langchain-ai.github.io/langgraph/concepts/streaming/): 'LLM should read this page when building a responsive app with real-time updates for workflow progress, LLM tokens generation, and custom updates.' 'Summary of key streaming concepts including modes (values, updates, messages-tuple, debug), streaming graph outputs (.stream and .astream), and LangGraph Platform streaming options.'

[Template Applications](https://langchain-ai.github.io/langgraph/concepts/template_applications/): LLM should read this page when: 1. An LLM is asked to generate a new LangGraph application from scratch. 2. An LLM needs information on the available templates and their descriptions. 3. An LLM requires instructions on how to create and deploy a LangGraph app using the CLI. The page provides information on template applications, installation of the LangGraph CLI, available templates, creating a new app from a template, next steps, and resources for learning more about the LangGraph platform.

[Time Travel ⏱️](https://langchain-ai.github.io/langgraph/concepts/time-travel/): LLM should read this page when an agent powered by a large language model (LLM) is experiencing errors or producing suboptimal results, needs to analyze decision-making processes, or wants to test alternative paths in a non-deterministic system. This page provides instructions on Time Travel, a technique for debugging and exploring agent decision-making, including replaying actions before a specific checkpoint and forking to create new branches within the graph.

