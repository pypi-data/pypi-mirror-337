- label: "模型类型选择"
  options:
    - "一见多模态大模型-Pro"
  name: "networkArchitecture"
  defaultIndex: 0
  help: "在多模态大模型中，不同规模模型的参数数量不同，Pro能够处理更复杂的任务，生成更高质量的结果"
  type: "Select"

- label: "训练轮数"
  min: 1
  max: 100
  # defaultValue: 80
  defaultValue: 10
  step: 1
  name: "epoch"
  help: "训练轮数 (Epochs) 指每个样本被训练的次数；⽀持输⼊1～100的整数"
  type: "inputNumber"

- label: "数据读取进程数量"
  min: 1
  max: 8
  defaultValue: 2
  step: 1
  name: "workerNum"
  help: "多卡训练中，每一张卡数据读取时使用的并行进程数量，参数越大占用内存越多；⽀持输⼊1～8的整数"
  type: "inputNumber"

- label: "批次大小"
  min: 1
  max: 16
  defaultValue: 1
  step: 1
  name: "batchSize"
  help: "批次⼤⼩ (Batch Size) 是指⼀次训练(Iteration)所包含的样本数，BatchSize越⼤，显存占⽤越⼤；⽀持输⼊1～16的整数"
  type: "inputNumber"

- label: "基础学习率"
  min: 0.00001
  max: 0.5
  defaultValue: 0.0001
  step: 0.00001
  name: "learning_rate"
  formatter: "%f"
  help: "基础学习率 (Learning Rate) 控制了模型参数更新步⻓，⾼学习率可能跳过最优解，低学习率收敛慢；⽀持输⼊0.00001～0.5的数字"
  type: "inputNumber"

- label: "梯度检查点"
  options:
    - "True"
    - "False"
  name: "gradient_checkpointing"
  defaultIndex: 0
  help: "梯度检查点 (Gradient Checkpointing) 旨在减少内存使⽤和提⾼⼤型模型的可训练性；开启可以节约显存, 但会略微降低训练速度"
  type: "Select"

- label: "最⼤序列⻓度"
  min: 500
  max: 8192
  defaultValue: 2048
  step: 1
  name: "max_length"
  help: "最⼤序列⻓度 (Max Length) 指单个训练数据样本的最⼤⻓度，超出配置⻓度将丢弃，⽀持输⼊500～8192的整数"
  type: "inputNumber"

- label: "输出截断策略"
  options:
    - "delete"
    - "left"
    - "right"
  name: "truncation_strategy"
  defaultIndex: 0
  help: "输出截断策略 (Truncation Strategy) 指在处理数据时单样本的tokens超过最⼤序列⻓度处理策略；支持删除、左侧裁剪和右侧裁剪"
  type: "Select"

- label: "LoRA秩值"
  options:
    - "8"
    - "16"
    - "32"
    - "64"
  name: "lora_rank"
  defaultIndex: 0
  help: "LoRA秩值 (LoRA Rank) 指LoRA训练中的秩⼤⼩，LoRA的核心思想是通过低秩分解来近似权重更新，秩决定了低秩矩阵的维度，rank 越大，模型表达能力越强，但计算量和参数量也增加"
  type: "Select"

- label: "LoRA阿尔法"
  options:
    - "8"
    - "16"
    - "32"
    - "64"
  name: "lora_alpha"
  defaultIndex: 2
  help: "LoRA阿尔法 (LoRA Alpha) 指训练中的缩放系数，是LoRA更新矩阵的缩放因子，用于控制低秩更新的幅度"
  type: "Select"

- label: "LoRA丢弃率"
  min: 0.00001
  max: 0.2
  defaultValue: 0.05
  step: 0.01
  name: "lora_dropout"
  formatter: "%f"
  help: "LoRA丢弃率 (LoRA DropOut) 是一种正则化技术，随机丢弃部分神经元输出以防止过拟合，提⾼模型泛化能⼒；⽀持输⼊0.00001～0.20的数字"
  type: "inputNumber"

- label: "LoRA权重初始化⽅式"
  options:
    - "true"
    - "false"
    - "gaussian"
    - "pissa"
  name: "init_weights"
  defaultIndex: 0
  help: "LoRA权重初始化⽅式 (Init LoRA Weights) 决定LoRA权重初始化⽅式"
  type: "Select"

- label: "LoRA权重偏置可训练"
  options:
    - "none"
    - "all"
  name: "lora_bias"
  defaultIndex: 0
  help: "LoRA权重偏置可训练 (LoRA Bias Trainable) 决定LoRA权重偏置是否可训练"
  type: "Select"

- label: "LoRA权重数据类型"
  options:
    - "None"
    - "float16"
    - "bfloat16"
    - "float32"
  name: "lora_dtype"
  defaultIndex: 0
  help: "LoRA权重数据类型 (LoRA Dtype)指LoRA权重数据类型；支持float16、bfloat16、float32。默认为None，跟随原模型类型"
  type: "Select"