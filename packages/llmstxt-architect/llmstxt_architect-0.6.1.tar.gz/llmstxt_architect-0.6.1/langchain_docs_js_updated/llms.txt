[API Reference](https://api.js.langchain.com/): LLM should read this page when learning about LangChain.js framework, understanding its core components, or exploring how to build LLM-powered applications. (LangChain.js is a TypeScript framework for developing applications powered by language models. It offers composable tools and integrations across Model I/O, Retrieval, and Agents modules. The ecosystem includes multiple packages (@langchain/core, @langchain/community, langchain, LangGraph.js), supports various environments, and enables context-aware applications that can reason using LLMs.)

[Document](https://api.js.langchain.com/classes/_langchain_core.documents.Document.html/): LLM should read this page when needing details about Document class implementation in LangChain.js, understanding document structure in LangChain, or implementing document handling with metadata. (Documentation for the Document class in @langchain/core, which is the interface for interacting with documents, showing its constructor, properties including pageContent, metadata, and optional id field, and type parameters for metadata customization)

[StructuredTool](https://api.js.langchain.com/classes/_langchain_core.tools.StructuredTool.html/): LLM should read this page when (needing to create structured tools for LangChain, understanding StructuredTool implementation details, examining tool input validation patterns) (Core documentation for the abstract StructuredTool class, which is the base class for LangChain tools accepting input defined by Zod schemas, including properties like name, description, schema, methods for invoking tools, and configuration options)

[Conceptual Guides](https://js.langchain.com/docs/concepts/): LLM should read this page when needing to understand LangChain's conceptual framework, learning about key components, or finding definitions of LangChain terminology. (A comprehensive guide to LangChain concepts including chat models, messages, tools, tool calling, structured output, runnables, streaming, LCEL, retrieval, embedding models, vector stores, agents, and more with a detailed glossary of LangChain terms and components.)

[Agents](https://js.langchain.com/docs/concepts/agents/): LLM should read this page when (needing to understand agent capabilities in LangChain, migrating from AgentExecutor to LangGraph, or seeking resources about agent implementation) (Explains that agents are systems that use LLMs as reasoning engines to take actions, recommends using LangGraph for building agents instead of the legacy AgentExecutor, and provides links to migration guides and documentation resources for both approaches)

[@langchain/langgraph](https://js.langchain.com/docs/concepts/architecture/#langchainlanggraph): LLM should read this page when needing to understand LangChain's framework structure, looking for package dependencies, or planning integration implementations. (Overview of LangChain's architecture showing the hierarchical organization with @langchain/core providing base abstractions, langchain containing cognitive architecture components, integration packages for third-party services, @langchain/community for community-maintained integrations, @langchain/langgraph for orchestration, and LangSmith for debugging and monitoring LLM applications.)

[Callbacks](https://js.langchain.com/docs/concepts/callbacks/): LLM should read this page when learning about LangChain's callback system, implementing event tracking in LLM applications, or customizing monitoring for chains and tools. (Comprehensive guide to LangChain's callback system explaining how to hook into various stages of LLM applications. Covers callback events, callback handlers, and how to pass callbacks at both constructor and request time. Includes details on event types like model starts/ends, token generation, chain events, tool events, and retriever events.)

[Managing chat history](https://js.langchain.com/docs/concepts/chat_history/#managing-chat-history): LLM should read this page when needing to understand chat history structures, implementing conversation management in LLM applications, or configuring proper message sequences for tool calling. (This page explains chat history concepts in LangChain, including conversation patterns between users and assistants, proper message sequencing, managing history to avoid context window limits, and guidelines for maintaining correct conversation structure with system, user, assistant, and tool messages.)

[Standard parameters for chat models](https://js.langchain.com/docs/concepts/chat_models/#standard-parameters): LLM should read this page when (needing to understand chat model interfaces in LangChain, implementing tool calling with LLMs, or requiring information about structured outputs from models) (Comprehensive guide to chat models in LangChain covering the model interface, key methods, standard parameters, tool calling capabilities, structured outputs, multimodality, context windows, and advanced topics like caching. Includes information on integrations with various providers and the standard message format system.)

[Document Loaders](https://js.langchain.com/docs/concepts/document_loaders/): LLM should read this page when needing to load documents from external sources, understanding document loader concepts, or implementing data ingestion pipelines. (Document loaders are components that load data from various sources into Document objects for LLM processing. The page explains the loader interface with .load() and .lazy_load() methods, provides code examples, and points to numerous integrations with services like Slack, Notion, and Google Drive.)

[Embedding Models](https://js.langchain.com/docs/concepts/embedding_models/): LLM should read this page when (needing to understand embedding models for vector search, implementing retrieval systems, or comparing similarity between text samples) (Conceptual overview of embedding models explaining how they transform text into numerical vectors, their historical evolution from BERT to modern implementations, how to use LangChain's embedding interface methods, and common similarity metrics like cosine similarity for comparing embeddings)

[Evaluation](https://js.langchain.com/docs/concepts/evaluation/): LLM should read this page when needing to understand evaluation methods for LLM applications, searching for tools to assess model performance, or learning how to implement testing frameworks. (This page explains LangChain's evaluation concepts for assessing LLM application performance. It highlights how LangSmith assists with creating datasets, defining metrics, running evaluations, and tracking results over time. The page emphasizes the importance of evaluation for building reliable LLM applications and points to more detailed documentation in LangSmith.)

[Example Selectors](https://js.langchain.com/docs/concepts/example_selectors/): LLM should read this page when (learning about dynamic example selection for few-shot prompting, needing to understand example selector concepts, implementing context-aware example selection) (Overview of Example Selectors which dynamically choose and format examples for prompts, improving LLM performance through few-shot prompting rather than using hardcoded examples, with links to related how-to guides and API reference)

[Few-shot prompting](https://js.langchain.com/docs/concepts/few_shot_prompting/): LLM should read this page when wanting to learn about few-shot prompting techniques, improving model performance with examples, or implementing effective example selection strategies. (This page explains few-shot prompting in 4 key areas: generating good examples (manual, better model, feedback), determining optimal example quantity, selecting examples (random, similarity-based), and formatting examples properly in prompts. It covers both single-turn and multi-turn examples, considerations for tool-calling, and best practices for different models.)

[LangChain Expression Language (LCEL)](https://js.langchain.com/docs/concepts/lcel/): LLM should read this page when needing to understand LangChain Expression Language, looking for ways to optimize chain execution, or deciding between LCEL and LangGraph. (LCEL is a declarative approach for building chains from Runnables, offering benefits including optimized parallel execution, simplified streaming, LangSmith tracing, a standard API, and LangServe deployment. Key composition primitives include RunnableSequence for sequential operations and RunnableParallel for concurrent execution, with shorthand syntax like the pipe method for improved readability.)

[ToolMessage](https://js.langchain.com/docs/concepts/messages/#toolmessage): LLM should read this page when (needing to understand message types in LangChain, implementing chat interfaces, or working with different message roles) (Comprehensive guide to Messages in LangChain, covering message roles (system, user, assistant, tool), message content types, standardized message structure, conversation formats, and how different message types like SystemMessage, HumanMessage, AIMessage, ToolMessage and AIMessageChunk are implemented)

[Multimodality](https://js.langchain.com/docs/concepts/multimodality/): LLM should read this page when (seeking to understand multimodal capabilities in LangChain, implementing models that process both text and image inputs, or developing applications that need to handle various data types) (Conceptual guide to multimodality in LangChain covering how chat models can accept and process multiple data formats like text and images, current limitations of multimodal outputs and tool use, and the future potential for multimodal embedding models and vector stores)

[Output Parsers](https://js.langchain.com/docs/concepts/output_parsers/): LLM should read this page when needing to parse structured outputs from language models, when handling JSON/XML responses from LLMs, or when selecting appropriate output parsers for specific data formats. (This page describes various output parsers in LangChain that transform LLM outputs into structured formats. It covers JSON, XML, CSV, OutputFixing, Datetime, and Structured parsers, with details about their streaming support, format instructions, and use cases. The page notes that function/tool calling is recommended over output parsing when possible.)

[Prompt Templates](https://js.langchain.com/docs/concepts/prompt_templates/): LLM should read this page when needing to understand prompt templates in LangChain, implementing different types of prompts, or working with message placeholders. (This page explains LangChain's prompt templates that help translate user inputs into instructions for language models. It covers three main types: String PromptTemplates for formatting single strings, ChatPromptTemplates for formatting lists of messages, and MessagesPlaceholder for inserting lists of messages at specific positions in prompts.)

[Retrieval Augmented Generation (RAG)](https://js.langchain.com/docs/concepts/rag/): LLM should read this page when needing to understand RAG concepts, implementing a retrieval system for LLMs, or designing knowledge augmentation solutions. (Conceptual overview of Retrieval Augmented Generation (RAG), explaining how it enhances language models by retrieving and incorporating external knowledge into prompts, with details on retrieval systems, benefits of RAG including up-to-date information access and reduced hallucinations, and implementation patterns with code examples.)

[Retrieval](https://js.langchain.com/docs/concepts/retrieval/): LLM should read this page when (seeking to understand different retrieval systems, implementing query analysis techniques, or building a RAG application) (Comprehensive guide on retrieval systems that covers key concepts like query analysis and information retrieval. Explains techniques for query re-writing and construction, and details various retrieval systems including lexical search indexes, vector indexes, relational databases, and graph databases. Provides implementation examples for specific approaches like decomposition, step-back prompting, HyDE, and self-query retrieval.)

[Retrievers](https://js.langchain.com/docs/concepts/retrievers/): LLM should read this page when (needing to understand how retrievers work in LangChain, implementing RAG systems, or comparing different retrieval approaches) (Explains retrievers in LangChain which provide a standard interface for retrieving documents using natural language queries. Covers the basic retriever interface, common types including search APIs, relational/graph databases, lexical search, and vector stores. Discusses advanced patterns like ensemble retrievers and source document retention strategies including ParentDocument and MultiVector retrievers.)

[RunnableConfig](https://js.langchain.com/docs/concepts/runnables/#runnableconfig): LLM should read this page when (needing to understand the Runnable interface in LangChain, implementing custom processing logic in LCEL chains, or configuring execution parameters for LangChain components) (Comprehensive guide to the Runnable interface which is fundamental to LangChain components. Covers core methods like invoke, batch, stream and streamEvents; explains RunnableConfig options including runName, tags, callbacks, and concurrency settings; details input/output types across components; demonstrates how to create custom Runnables with RunnableLambda and RunnableGenerator functions)

[stream](https://js.langchain.com/docs/concepts/streaming/): LLM should read this page when looking for ways to implement real-time output streaming in LLM applications, when needing to understand different streaming APIs in LangChain, or when troubleshooting performance issues with streaming implementations. (The page explains streaming in LangChain, covering what data to stream (LLM outputs, workflow progress, custom data), the main streaming APIs (stream() and streamEvents()), how to write custom data to streams, auto-streaming features for chat models, and links to related resources and how-to guides.)

[withStructuredOutput](https://js.langchain.com/docs/concepts/structured_outputs/#structured-output-method): LLM should read this page when needing to implement structured outputs with LLMs, understanding schema definition methods, or configuring models to return data in specific formats. (Comprehensive guide to structured outputs in LangChain, covering schema definition using Zod or JSONSchema, implementation methods including tool calling and JSON mode, and the recommended .withStructuredOutput() method that simplifies binding schemas to models and parsing responses.)

[Text Splitters](https://js.langchain.com/docs/concepts/text_splitters/): LLM should read this page when (looking for document chunking strategies, implementing text splitting for RAG applications, or overcoming model context limitations) (Comprehensive guide to text splitters in LangChain, covering why document splitting is necessary and four main approaches: length-based splitting (character/token), text-structure based (recursive splitting), document-structure based (for HTML/markdown/code), and semantic meaning based splitting. Includes code examples and explanations of when to use each approach.)

[Tokens](https://js.langchain.com/docs/concepts/tokens/): LLM should read this page when needing to understand how language models process text, explaining tokenization to users, or working with token-related code and limitations. (Explains what tokens are in LLMs - the basic processing units that can represent words, word parts, or other linguistic elements. Covers tokenization process, how tokens enable better language understanding than character-level processing, token counting metrics, and notes that tokens can also represent non-text modalities like images or audio.)

[Tool binding](https://js.langchain.com/docs/concepts/tool_calling/#tool-binding): LLM should read this page when (needing to understand how tool calling works in LangChain, implementing LLM-powered function execution, or developing applications that require structured outputs from models) (This page explains tool calling in LangChain, covering the four key concepts: tool creation using the tool() function, tool binding to models via bindTools(), tool calling where models decide when to use tools and return structured arguments, and tool execution using the Runnable interface. It includes code examples, best practices, and diagrams illustrating the process flow.)

[Toolkits](https://js.langchain.com/docs/concepts/tools/#toolkits): LLM should read this page when learning about tool creation in LangChain, implementing function calling with LLMs, or building agents that use external functions. (Comprehensive guide to LangChain's tool abstraction which associates TypeScript functions with schemas, allowing models to call functions with specific inputs. Covers tool creation with the 'tool' function, tool interfaces, schemas, tool artifacts, runtime configurations, best practices, and toolkits.)

[Tracing](https://js.langchain.com/docs/concepts/tracing/): LLM should read this page when needing to understand tracing capabilities in LangChain or when troubleshooting observability issues in LangChain applications. (Tracing in LangChain records the series of steps from input to output, capturing individual runs from models, retrievers, tools, or sub-chains to provide observability for diagnosing issues in chains and agents.)

[Vector stores](https://js.langchain.com/docs/concepts/vectorstores/): LLM should read this page when (needing to understand vector stores for RAG applications, implementing semantic search capabilities, or working with document embeddings) (Vector stores are specialized data stores for indexing and retrieving information using vector representations called embeddings. The page covers the standard interface for working with vector stores including initialization, adding documents, deleting documents, and conducting similarity searches. It explains similarity metrics, search algorithms, metadata filtering, and advanced techniques like Maximal Marginal Relevance to improve search quality and diversity.)

[Why LangChain?](https://js.langchain.com/docs/concepts/why_langchain/): LLM should read this page when needing to understand LangChain's purpose, when evaluating if LangChain is suitable for an AI application, or when exploring LangChain ecosystem components. (This page explains why developers should use LangChain, highlighting its three main features: standardized component interfaces that make it easy to switch between providers, orchestration capabilities for complex applications through LangGraph, and observability/evaluation tools via LangSmith to monitor and improve applications.)

[How-to guides](https://js.langchain.com/docs/how_to/): LLM should read this page when (needing to find specific implementation guides for LangChain.js, looking for solutions to common LLM application problems, or planning how to implement particular features in a LangChain project) (Comprehensive index of how-to guides for LangChain.js covering core functionality including installation, LCEL, components (prompts, models, retrievers, etc.), and use cases like RAG, extraction, chatbots, and SQL/graph database integration, with links to related technologies like LangGraph.js and LangSmith)

[How to: install LangChain packages](https://js.langchain.com/docs/how_to/installation/): LLM should read this page when (wanting to install LangChain in a JavaScript/TypeScript project, troubleshooting version conflicts, or determining compatible environments) (This page covers LangChain installation instructions for various environments (Node.js, Cloudflare, Vercel/Next.js, Deno, Browser), explains how to install core and integration packages, provides solutions for dependency conflicts with @langchain/core, and offers guidance for different module systems (ESM, CommonJS))

[How to: stream runnables](https://js.langchain.com/docs/how_to/streaming/): LLM should read this page when (implementing streaming functionality with LangChain JS, troubleshooting streaming issues in LLM applications, or learning how to display partial outputs while an LLM generates responses) (Comprehensive guide on streaming in LangChain JS, covering two main approaches: .stream() for final output streaming and streamEvents() for intermediate steps streaming. Includes code examples for working with Chat Models, handling JSON outputs, filtering events, handling non-streaming components, and implementing streaming over HTTP connections.)

[How to: get models to return structured output](https://js.langchain.com/docs/how_to/structured_output/): LLM should read this page when (wanting to extract structured data from LLM outputs) (building applications that require specific JSON schema responses) (needing to parse LLM responses into a required format) (This page covers methods for getting structured data from models, including using the .withStructuredOutput() method with Zod or JSON schema, different output methods like function calling and JSON mode, handling raw outputs, and alternate approaches using prompting techniques with JsonOutputParser or custom parsing functions)

[How to: use chat models to call tools](https://js.langchain.com/docs/how_to/tool_calling/): LLM should read this page when (needing to implement tool calling with chat models, integrating LLM-powered tools into applications, troubleshooting tool call formats) (Guide explaining how to use chat models to call tools, including binding LangChain tool objects to models, handling tool call responses, supporting different model providers like Anthropic/OpenAI/Mistral, and formatting model-specific tool schemas)

[chat model integrations](https://js.langchain.com/docs/integrations/chat/): LLM should read this page when (looking for chat model integrations, comparing features of different chat model providers, or implementing a chat model in a LangChain application) (Comprehensive directory of chat model integrations in LangChain, including installation instructions, environment setup, and feature comparison across providers like OpenAI, Anthropic, Groq, Mistral, Google and many others with details about their capabilities like streaming, JSON mode, tool calling and multimodal support)

[ChatAnthropic](https://js.langchain.com/docs/integrations/chat/anthropic/): LLM should read this page when (integrating Anthropic Claude models with LangChain, using citations with Anthropic models, or implementing prompt caching) (Documentation for the ChatAnthropic integration in LangChain.js, covering model setup, credentials, installation, invocation, content blocks, tool calling, custom headers, prompt caching, custom clients, and citation features)

[BedrockChat](https://js.langchain.com/docs/integrations/chat/bedrock/): LLM should read this page when (looking for information about Amazon Bedrock Chat integration in LangChain, setting up AWS Bedrock credentials, or implementing tool calling with Bedrock models) (Documentation for BedrockChat integration in LangChain covering installation, authentication with AWS, model instantiation, invocation methods, chaining with prompt templates, tool calling capabilities, and supported model features including token-level streaming and image input)

[ChatBedrockConverse](https://js.langchain.com/docs/integrations/chat/bedrock_converse/): LLM should read this page when (needing to implement AWS Bedrock Converse chat models, looking for Amazon Bedrock conversational interface information, or configuring AWS chat models with tool calling capabilities) (Guide to using ChatBedrockConverse which provides a unified conversational interface for Amazon Bedrock foundation models, including setup instructions, authentication methods, model invocation examples, chaining techniques, and support for tool calling, structured output, and image inputs)

[ChatCloudflareWorkersAI](https://js.langchain.com/docs/integrations/chat/cloudflare_workersai/): LLM should read this page when (seeking to integrate with Cloudflare Workers AI chat models, implementing Cloudflare AI in LangChain applications, or building applications that need to run ML models on Cloudflare's network) (This page documents the ChatCloudflareWorkersAI integration for LangChain, covering model features, credentials setup, installation, instantiation, invocation examples, chaining with prompt templates, and API reference links. The integration supports image input and token-level streaming but not tool calling or structured output.)

[ChatCohere](https://js.langchain.com/docs/integrations/chat/cohere/): LLM should read this page when (looking for information about using Cohere's chat models in LangChain.js, implementing RAG with Cohere, or using Cohere's web search connector) (Documentation for ChatCohere integration with LangChain.js including setup, credentials, model instantiation, basic invocation, chaining, RAG capabilities with documents as context, and web search connector functionality)

[ChatFireworks](https://js.langchain.com/docs/integrations/chat/fireworks/): LLM should read this page when needing to integrate with Fireworks AI chat models, when implementing token-level streaming with Fireworks, or when setting up tool calling with Fireworks models. (Documentation for ChatFireworks integration in LangChain, including model features, setup instructions, API credentials, installation, basic usage examples, chaining with prompt templates, and references to advanced features like tool calling, structured output, and token usage tracking)

[ChatGoogleGenerativeAI](https://js.langchain.com/docs/integrations/chat/google_generativeai/): LLM should read this page when (working with Google Generative AI/Gemini models, implementing tool calling with Google AI, setting up multimodal inputs or search retrieval tools) (Detailed guide to ChatGoogleGenerativeAI integration in LangChain, covering setup, authentication, instantiation, calling methods, safety settings, tool calling, built-in Google Search retrieval, code execution capabilities, context caching, and model limitations)

[ChatVertexAI](https://js.langchain.com/docs/integrations/chat/google_vertex_ai/): LLM should read this page when (setting up Google Vertex AI chat models in LangChain, implementing tool calling with Google Search retrieval, or configuring context caching with Vertex AI) (ChatVertexAI provides integration with Google's Vertex AI service for accessing models like Gemini. The page covers setup, credentials, invocation examples, tool calling with Google Search retrieval, context caching, and model chaining. It details model features including tool calling, structured output, multimodal inputs, and token usage tracking.)

[ChatGroq](https://js.langchain.com/docs/integrations/chat/groq/): LLM should read this page when needing to implement Groq's chat models in LangChain, when setting up API connections to Groq, or when configuring specific features like JSON mode or tool calling. (This page provides comprehensive documentation for integrating ChatGroq with LangChain JS/TS, including installation instructions, authentication setup, model instantiation, basic invocation patterns, JSON response formatting, chaining with prompts, and supported model features like tool calling and structured output.)

[ChatMistralAI](https://js.langchain.com/docs/integrations/chat/mistral/): LLM should read this page when (finding documentation about integrating with MistralAI chat models in LangChain, troubleshooting MistralAI implementation, or exploring tool calling capabilities with MistralAI) (This page documents the ChatMistralAI integration in LangChain, covering setup requirements, instantiation, invocation methods, chaining with templates, tool calling implementation, custom hooks, and model features including token usage tracking and image input support)

[ChatOllama](https://js.langchain.com/docs/integrations/chat/ollama/): LLM should read this page when (looking to integrate Ollama's open-source LLMs locally, implementing tool calling with Ollama models, or working with multimodal capabilities in Ollama) (ChatOllama integration documentation covering installation, model instantiation, invocation, chaining with prompt templates, tool calling functionality, JSON mode, structured output, and multimodal image input capabilities for local LLM deployment)

[ChatOpenAI](https://js.langchain.com/docs/integrations/chat/openai/): LLM should read this page when (looking for details on ChatOpenAI integration, setting up OpenAI chat models, or implementing advanced OpenAI features like tool calling) (Comprehensive guide to using OpenAI chat models in LangChain, covering installation, authentication, basic usage, advanced features like tool calling, structured output, custom URLs, headers, audio capabilities, and the Responses API for built-in web/file search and conversation state management)

[ChatTogetherAI](https://js.langchain.com/docs/integrations/chat/togetherai/): LLM should read this page when (implementing TogetherAI chat models in LangChain, using open-source models via Together API, or adding multimodal capabilities to LangChain projects) (Documentation for ChatTogetherAI integration in LangChain, covering installation, API key setup, model instantiation, invocation, chaining, and supported features including tool calling, structured output, JSON mode, and multimodal inputs)

[ChatXAI](https://js.langchain.com/docs/integrations/chat/xai/): LLM should read this page when needing integration details for xAI's Grok model, implementing ChatXAI in JavaScript/TypeScript, or configuring tool calling with xAI. (Documentation for the ChatXAI integration in LangChain JS, covering model features, setup, credentials, installation, instantiation, invocation, chaining functionality, and API references for the xAI Grok model)

[Tutorials](https://js.langchain.com/docs/tutorials/): LLM should read this page when looking for LangChain tutorial resources, needing to get started with LLM application development, or searching for structured learning paths in LangChain. (The page contains a comprehensive collection of tutorials organized into three main sections: "Get started" tutorials covering basic components like chat models and prompts, "Orchestration" tutorials for building complex applications with LangGraph, and "LangSmith" tutorials for evaluation and monitoring of LLM applications.)

[Memory](https://langchain-ai.github.io/langgraphjs/concepts/memory/): LLM should read this page when needing to understand memory management in LangGraph, implementing short-term or long-term memory in agents, or handling long conversation histories. (Comprehensive guide to memory in LangGraph, covering both short-term memory (within conversation threads) and long-term memory (across threads), with techniques for managing long conversations, editing message lists, summarizing past conversations, writing/managing memories, and representing memories as instructions or few-shot examples.)

