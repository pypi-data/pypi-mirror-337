<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ü§ñ Analyze and Test robots.txt Files on a Large Scale &mdash;  Python</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=81998473"></script>
        <script src="_static/doctools.js?v=9bcbadda"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
        <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
        <script async="async" src="https://unpkg.com/thebe@0.8.2/lib/index.js"></script>
        <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Download, Parse, and Analyze XML Sitemaps" href="advertools.sitemaps.html" />
    <link rel="prev" title="Create Ads Using Long Descriptive Text (top-down approach)" href="advertools.ad_from_string.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            advertools
          </a>
              <div class="version">
                0.16.4
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="readme.html">About advertools</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.survey.html">Survey - your feedback</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.cli.cli.html">Command Line Interface</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SEM</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.kw_generate.html">Generate SEM Keywords</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_create.html">Create Text Ads on a Large Scale</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.ad_from_string.html">Create Text Ads From Description Text</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">SEO</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">robots.txt</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bulk-robots-txt-tester">Bulk <code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> Tester</a></li>
<li class="toctree-l2"><a class="reference internal" href="#user-agents">User-agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robots-txt-testing-approach">robots.txt Testing Approach</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="advertools.sitemaps.html">XML Sitemaps</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.spider.html">SEO Spider / Crawler</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.code_recipes.spider_strategies.html">Crawl Strategies</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.crawlytics.html">Crawl Analytics</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.header_spider.html">Crawl headers (HEAD method only)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.image_spider.html">Crawl images</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.logs.html">Python Log File Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.logs.html#parse-and-analyze-crawl-logs-in-a-dataframe">Parse and Analyze Crawl Logs in a Dataframe</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.reverse_dns_lookup.html">Reverse DNS Lookup</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.serp.html">Analyze Search Engine Results (SERPs)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.knowledge_graph.html">Google's Knowledge Graph</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Text &amp; Content Analysis</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.urlytics.html">URL Structure Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.emoji.html">Emoji Tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.extract.html">Extract Structured Entities from Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.stopwords.html">Stop Words</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_frequency.html">Text Analysis (absolute &amp; weighted word frequency)</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.word_tokenize.html">Word Tokenization (N-grams)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Social Media</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="advertools.twitter.html">Twitter Data API</a></li>
<li class="toctree-l1"><a class="reference internal" href="advertools.youtube.html">YouTube Data API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index &amp; Change Log</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="include_changelog.html">Index &amp; Change Log</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">advertools</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">ü§ñ Analyze and Test robots.txt Files on a Large Scale</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/advertools.robotstxt.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="analyze-and-test-robots-txt-files-on-a-large-scale">
<span id="robotstxt"></span><span id="module-advertools.robotstxt"></span><h1>ü§ñ Analyze and Test robots.txt Files on a Large Scale<a class="headerlink" href="#analyze-and-test-robots-txt-files-on-a-large-scale" title="Link to this heading">ÔÉÅ</a></h1>
<p>Even though they are tiny in size, robots.txt files contain potent instructions
that can block major sections of your site, which is what they are supposed to
do. Only sometimes you might make the mistake of blocking the wrong section.</p>
<p>So it is very important to check if certain pages (or groups of pages) are
blocked for a certain user-agent by a certain robots.txt file. Ideally, you
would want to run the same check for all possible user-agents. Even more
ideally, you want to be able to run the check for a large number of pages with
every possible combination with user-agents.</p>
<p>To get the robots.txt file into an easily readable format, you can use the
<a class="reference internal" href="#advertools.robotstxt.robotstxt_to_df" title="advertools.robotstxt.robotstxt_to_df"><code class="xref py py-func docutils literal notranslate"><span class="pre">robotstxt_to_df()</span></code></a> function to get it in a DataFrame.</p>
<button title="Run this code" class="thebelab-button thebe-launch-button" onclick="initThebe()">Run this code</button><div class="thebe thebe-init highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">advertools</span> <span class="k">as</span> <span class="nn">adv</span>

<span class="n">amazon</span> <span class="o">=</span> <span class="n">adv</span><span class="o">.</span><span class="n">robotstxt_to_df</span><span class="p">(</span><span class="s1">&#39;https://www.amazon.com/robots.txt&#39;</span><span class="p">)</span>
<span class="n">amazon</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>directive</p></th>
<th class="head"><p>content</p></th>
<th class="head"><p>etag</p></th>
<th class="head"><p>robotstxt_last_modified</p></th>
<th class="head"><p>robotstxt_url</p></th>
<th class="head"><p>download_date</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>User-agent</p></td>
<td><p>*</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Disallow</p></td>
<td><p>/exec/obidos/account-access-login</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Disallow</p></td>
<td><p>/exec/obidos/change-style</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Disallow</p></td>
<td><p>/exec/obidos/flex-sign-in</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Disallow</p></td>
<td><p>/exec/obidos/handle-buy-box</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
</tr>
<tr class="row-even"><td><p>146</p></td>
<td><p>Disallow</p></td>
<td><p>/hp/video/mystuff</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>147</p></td>
<td><p>Disallow</p></td>
<td><p>/gp/video/profiles</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-even"><td><p>148</p></td>
<td><p>Disallow</p></td>
<td><p>/hp/video/profiles</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>149</p></td>
<td><p>User-agent</p></td>
<td><p>EtaoSpider</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
<tr class="row-even"><td><p>150</p></td>
<td><p>Disallow</p></td>
<td><p>/</p></td>
<td><p>&quot;a850165d925db701988daf7ead7492d3&quot;</p></td>
<td><p>2021-10-28 17:51:39+00:00</p></td>
<td><p><a class="reference external" href="https://www.amazon.com/robots.txt">https://www.amazon.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:33:03.200689+00:00</p></td>
</tr>
</tbody>
</table>
<p>The returned DataFrame contains columns for directives, their content, the URL
of the robots.txt file, as well as the date it was downloaded.</p>
<ul class="simple">
<li><p><cite>directive</cite>: The main commands. Allow, Disallow, Sitemap, Crawl-delay,
User-agent, and so on.</p></li>
<li><p><cite>content</cite>: The details of each of the directives.</p></li>
<li><p><cite>robotstxt_last_modified</cite>: The date when the robots.txt file was last
modified, if provided (according the response header Last-modified).</p></li>
<li><p><cite>etag</cite>: The entity tag of the response header, if provided.</p></li>
<li><p><cite>robotstxt_url</cite>: The URL of the robots.txt file.</p></li>
<li><p><cite>download_date</cite>: The date and time when the file was downloaded.</p></li>
</ul>
<p>Alternatively, you can provide a list of robots URLs if you want to download
them all in one go. This might be interesting if:</p>
<ul class="simple">
<li><p>You are analyzing an industry and want to keep an eye on many different
websites.</p></li>
<li><p>You are analyzing a website with many sub-domains, and want to get all the
robots files together.</p></li>
<li><p>You are trying to understand a company that has many websites under different
domains and sub-domains.</p></li>
</ul>
<p>In this case you simply provide a list of URLs instead of a single one.</p>
<button title="Run this code" class="thebelab-button thebe-launch-button" onclick="initThebe()">Run this code</button><div class="thebe thebe-init highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">robots_urls</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;https://www.google.com/robots.txt&#39;</span><span class="p">,</span>
               <span class="s1">&#39;https://twitter.com/robots.txt&#39;</span><span class="p">,</span>
               <span class="s1">&#39;https://facebook.com/robots.txt&#39;</span><span class="p">]</span>

<span class="n">googtwfb</span> <span class="o">=</span> <span class="n">adv</span><span class="o">.</span><span class="n">robotstxt_to_df</span><span class="p">(</span><span class="n">robots_urls</span><span class="p">)</span>

<span class="c1"># How many lines does each robots file have?</span>
<span class="n">googtwfb</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;robotstxt_url&#39;</span><span class="p">)[</span><span class="s1">&#39;directive&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">robotstxt_url</span>
<span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">facebook</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">robots</span><span class="o">.</span><span class="n">txt</span>      <span class="mi">541</span>
<span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">twitter</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">robots</span><span class="o">.</span><span class="n">txt</span>       <span class="mi">108</span>
<span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">www</span><span class="o">.</span><span class="n">google</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">robots</span><span class="o">.</span><span class="n">txt</span>    <span class="mi">289</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">directive</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</pre></div>
</div>
<div class="thebe thebe-init highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display the first five rows of each of the robots files:</span>
<span class="n">googtwfb</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;robotstxt_url&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>directive</p></th>
<th class="head"><p>content</p></th>
<th class="head"><p>robotstxt_last_modified</p></th>
<th class="head"><p>robotstxt_url</p></th>
<th class="head"><p>download_date</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>User-agent</p></td>
<td><p>*</p></td>
<td><p>2022-02-07 22:30:00+00:00</p></td>
<td><p><a class="reference external" href="https://www.google.com/robots.txt">https://www.google.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.375724+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Disallow</p></td>
<td><p>/search</p></td>
<td><p>2022-02-07 22:30:00+00:00</p></td>
<td><p><a class="reference external" href="https://www.google.com/robots.txt">https://www.google.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.375724+00:00</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>Allow</p></td>
<td><p>/search/about</p></td>
<td><p>2022-02-07 22:30:00+00:00</p></td>
<td><p><a class="reference external" href="https://www.google.com/robots.txt">https://www.google.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.375724+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>Allow</p></td>
<td><p>/search/static</p></td>
<td><p>2022-02-07 22:30:00+00:00</p></td>
<td><p><a class="reference external" href="https://www.google.com/robots.txt">https://www.google.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.375724+00:00</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>Allow</p></td>
<td><p>/search/howsearchworks</p></td>
<td><p>2022-02-07 22:30:00+00:00</p></td>
<td><p><a class="reference external" href="https://www.google.com/robots.txt">https://www.google.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.375724+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>289</p></td>
<td><p>comment</p></td>
<td><p>Google Search Engine Robot</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://twitter.com/robots.txt">https://twitter.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.461815+00:00</p></td>
</tr>
<tr class="row-even"><td><p>290</p></td>
<td><p>comment</p></td>
<td><p>==========================</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://twitter.com/robots.txt">https://twitter.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.461815+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>291</p></td>
<td><p>User-agent</p></td>
<td><p>Googlebot</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://twitter.com/robots.txt">https://twitter.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.461815+00:00</p></td>
</tr>
<tr class="row-even"><td><p>292</p></td>
<td><p>Allow</p></td>
<td><p>/?_escaped_fragment_</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://twitter.com/robots.txt">https://twitter.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.461815+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>293</p></td>
<td><p>Allow</p></td>
<td><p>/*?lang=</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://twitter.com/robots.txt">https://twitter.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.461815+00:00</p></td>
</tr>
<tr class="row-even"><td><p>397</p></td>
<td><p>comment</p></td>
<td><p>Notice: Collection of data on Facebook through automated means is</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://facebook.com/robots.txt">https://facebook.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.474456+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>398</p></td>
<td><p>comment</p></td>
<td><p>prohibited unless you have express written permission from Facebook</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://facebook.com/robots.txt">https://facebook.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.474456+00:00</p></td>
</tr>
<tr class="row-even"><td><p>399</p></td>
<td><p>comment</p></td>
<td><p>and may only be conducted for the limited purpose contained in said</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://facebook.com/robots.txt">https://facebook.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.474456+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>400</p></td>
<td><p>comment</p></td>
<td><p>permission.</p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://facebook.com/robots.txt">https://facebook.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.474456+00:00</p></td>
</tr>
<tr class="row-even"><td><p>401</p></td>
<td><p>comment</p></td>
<td><p>See: <a class="reference external" href="http://www.facebook.com/apps/site_scraping_tos_terms.php">http://www.facebook.com/apps/site_scraping_tos_terms.php</a></p></td>
<td><p>NaT</p></td>
<td><p><a class="reference external" href="https://facebook.com/robots.txt">https://facebook.com/robots.txt</a></p></td>
<td><p>2022-02-11 19:52:13.474456+00:00</p></td>
</tr>
</tbody>
</table>
<section id="bulk-robots-txt-tester">
<h2>Bulk <code class="docutils literal notranslate"><span class="pre">robots.txt</span></code> Tester<a class="headerlink" href="#bulk-robots-txt-tester" title="Link to this heading">ÔÉÅ</a></h2>
<p>This tester is designed to work on a large scale. The <a class="reference internal" href="#advertools.robotstxt.robotstxt_test" title="advertools.robotstxt.robotstxt_test"><code class="xref py py-func docutils literal notranslate"><span class="pre">robotstxt_test()</span></code></a>
function runs a test for a given robots.txt file, checking which of the
provided user-agents can fetch which of the provided URLs, paths, or patterns.</p>
<button title="Run this code" class="thebelab-button thebe-launch-button" onclick="initThebe()">Run this code</button><div class="thebe thebe-init highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">advertools</span> <span class="k">as</span> <span class="nn">adv</span>
<span class="n">adv</span><span class="o">.</span><span class="n">robotstxt_test</span><span class="p">(</span>
    <span class="n">robotstxt_url</span><span class="o">=</span><span class="s1">&#39;https://www.amazon.com/robots.txt&#39;</span><span class="p">,</span>
    <span class="n">user_agents</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Googlebot&#39;</span><span class="p">,</span> <span class="s1">&#39;baiduspider&#39;</span><span class="p">,</span> <span class="s1">&#39;Bingbot&#39;</span><span class="p">],</span>
    <span class="n">urls</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="s1">&#39;/hello&#39;</span><span class="p">,</span> <span class="s1">&#39;/some-page.html&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>As a result, you get a DataFrame with a row for each combination of
(user-agent, URL) indicating whether or not that particular user-agent can
fetch the given URL.</p>
<p>Some reasons why you might want to do that:</p>
<ul class="simple">
<li><p>SEO Audits: Especially for large websites with many URL patterns, and many
rules for different user-agents.</p></li>
<li><p>Developer or site owner about to make large changes</p></li>
<li><p>Interest in strategies of certain companies</p></li>
</ul>
</section>
<section id="user-agents">
<h2>User-agents<a class="headerlink" href="#user-agents" title="Link to this heading">ÔÉÅ</a></h2>
<p>In reality there are only two groups of user-agents that you need to worry
about:</p>
<ul class="simple">
<li><p>User-agents listed in the robots.txt file: For each one of those you need to
check whether or not they are blocked from fetching a certain URL
(or pattern).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*</span></code> all other user-agents: The <code class="docutils literal notranslate"><span class="pre">*</span></code> includes all other user-agents, so
checking the rules that apply to it should take care of the rest.</p></li>
</ul>
</section>
<section id="robots-txt-testing-approach">
<h2>robots.txt Testing Approach<a class="headerlink" href="#robots-txt-testing-approach" title="Link to this heading">ÔÉÅ</a></h2>
<ol class="arabic simple">
<li><p>Get the robots.txt file that you are interested in</p></li>
<li><p>Extract the user-agents from it</p></li>
<li><p>Specify the URLs you are interested in testing</p></li>
<li><p>Run the <a class="reference internal" href="#advertools.robotstxt.robotstxt_test" title="advertools.robotstxt.robotstxt_test"><code class="xref py py-func docutils literal notranslate"><span class="pre">robotstxt_test()</span></code></a> function</p></li>
</ol>
<button title="Run this code" class="thebelab-button thebe-launch-button" onclick="initThebe()">Run this code</button><div class="thebe thebe-init highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fb_robots</span> <span class="o">=</span> <span class="n">adv</span><span class="o">.</span><span class="n">robotstxt_to_df</span><span class="p">(</span><span class="s1">&#39;https://www.facebook.com/robots.txt&#39;</span><span class="p">)</span>
<span class="n">fb_robots</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>directive</p></th>
<th class="head"><p>content</p></th>
<th class="head"><p>robotstxt_url</p></th>
<th class="head"><p>download_date</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>comment</p></td>
<td><p>Notice: Collection of data on Facebook through automated means is</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>comment</p></td>
<td><p>prohibited unless you have express written permission from Facebook</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p>comment</p></td>
<td><p>and may only be conducted for the limited purpose contained in said</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p>comment</p></td>
<td><p>permission.</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p>comment</p></td>
<td><p>See: <a class="reference external" href="http://www.facebook.com/apps/site_scraping_tos_terms.php">http://www.facebook.com/apps/site_scraping_tos_terms.php</a></p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
</tr>
<tr class="row-even"><td><p>536</p></td>
<td><p>Allow</p></td>
<td><p>/ajax/pagelet/generic.php/PagePostsSectionPagelet</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>537</p></td>
<td><p>Allow</p></td>
<td><p>/careers/</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-even"><td><p>538</p></td>
<td><p>Allow</p></td>
<td><p>/safetycheck/</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-odd"><td><p>539</p></td>
<td><p>User-agent</p></td>
<td><ul class="simple">
<li></li>
</ul>
</td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
<tr class="row-even"><td><p>540</p></td>
<td><p>Disallow</p></td>
<td><p>/</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>2022-02-12 00:48:58.951053+00:00</p></td>
</tr>
</tbody>
</table>
<p>Now that we have downloaded the file, we can easily extract the list of
user-agents that it contains.</p>
<button title="Run this code" class="thebelab-button thebe-launch-button" onclick="initThebe()">Run this code</button><div class="thebe thebe-init highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fb_useragents</span> <span class="o">=</span> <span class="p">(</span><span class="n">fb_robots</span>
                 <span class="p">[</span><span class="n">fb_robots</span><span class="p">[</span><span class="s1">&#39;directive&#39;</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;User-agent&#39;</span><span class="p">]</span>
                 <span class="p">[</span><span class="s1">&#39;content&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">drop_duplicates</span><span class="p">()</span>
                <span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="n">fb_useragents</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;Applebot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;baiduspider&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Bingbot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Discordbot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;facebookexternalhit&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Googlebot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Googlebot-Image&#39;</span><span class="p">,</span>
 <span class="s1">&#39;ia_archiver&#39;</span><span class="p">,</span>
 <span class="s1">&#39;LinkedInBot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;msnbot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Naverbot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Pinterestbot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;seznambot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Slurp&#39;</span><span class="p">,</span>
 <span class="s1">&#39;teoma&#39;</span><span class="p">,</span>
 <span class="s1">&#39;TelegramBot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Twitterbot&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Yandex&#39;</span><span class="p">,</span>
 <span class="s1">&#39;Yeti&#39;</span><span class="p">,</span>
 <span class="s1">&#39;*&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Quite a long list!</p>
<p>As a small and quick test, I'm interested in checking the home page, a random
profile page (/bbc), groups and hashtags pages.</p>
<button title="Run this code" class="thebelab-button thebe-launch-button" onclick="initThebe()">Run this code</button><div class="thebe thebe-init highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">urls_to_test</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="s1">&#39;/bbc&#39;</span><span class="p">,</span> <span class="s1">&#39;/groups&#39;</span><span class="p">,</span> <span class="s1">&#39;/hashtag/&#39;</span><span class="p">]</span>
<span class="n">fb_test</span> <span class="o">=</span> <span class="n">robotstxt_test</span><span class="p">(</span><span class="s1">&#39;https://www.facebook.com/robots.txt&#39;</span><span class="p">,</span>
                         <span class="n">fb_useragents</span><span class="p">,</span> <span class="n">urls_to_test</span><span class="p">)</span>
<span class="n">fb_test</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>robotstxt_url</p></th>
<th class="head"><p>user_agent</p></th>
<th class="head"><p>url_path</p></th>
<th class="head"><p>can_fetch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>*</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>*</p></td>
<td><p>/bbc</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>*</p></td>
<td><p>/groups</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>*</p></td>
<td><p>/hashtag/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>4</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Applebot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
<td><p>...</p></td>
</tr>
<tr class="row-even"><td><p>75</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>seznambot</p></td>
<td><p>/hashtag/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>76</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>teoma</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>77</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>teoma</p></td>
<td><p>/bbc</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>78</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>teoma</p></td>
<td><p>/groups</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>79</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>teoma</p></td>
<td><p>/hashtag/</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p>For twenty user-agents and four URLs each, we received a total of eighty test
results. You can immediately see that all user-agents not listed (denoted by
<cite>*</cite> are not allowed to fetch any of the provided URLs).</p>
<p>Let's see who is and who is not allowed to fetch the home page.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fb_test</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;url_path== &quot;/&quot;&#39;</span><span class="p">)</span>
</pre></div>
</div>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"></th>
<th class="head"><p>robotstxt_url</p></th>
<th class="head"><p>user_agent</p></th>
<th class="head"><p>url_path</p></th>
<th class="head"><p>can_fetch</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>*</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Applebot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>8</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Bingbot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Discordbot</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>16</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Googlebot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>20</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Googlebot-Image</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>24</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>LinkedInBot</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>28</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Naverbot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>32</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Pinterestbot</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>36</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Slurp</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>40</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>TelegramBot</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>44</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Twitterbot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>48</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Yandex</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>52</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>Yeti</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>56</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>baiduspider</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>60</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>facebookexternalhit</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-even"><td><p>64</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>ia_archiver</p></td>
<td><p>/</p></td>
<td><p>False</p></td>
</tr>
<tr class="row-odd"><td><p>68</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>msnbot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-even"><td><p>72</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>seznambot</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
<tr class="row-odd"><td><p>76</p></td>
<td><p><a class="reference external" href="https://www.facebook.com/robots.txt">https://www.facebook.com/robots.txt</a></p></td>
<td><p>teoma</p></td>
<td><p>/</p></td>
<td><p>True</p></td>
</tr>
</tbody>
</table>
<p>I'll leave it to you to figure out why LinkedIn and Pinterest are not allowed
to crawl the home page but Google and Apple are, because I have no clue!</p>
</section>
</section>
<dl class="py function">
<dt class="sig sig-object py" id="advertools.robotstxt.robotstxt_test">
<span class="sig-name descname"><span class="pre">robotstxt_test</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">robotstxt_url</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">user_agents</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">urls</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/advertools/robotstxt.html#robotstxt_test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#advertools.robotstxt.robotstxt_test" title="Link to this definition">ÔÉÅ</a></dt>
<dd><p>Given a <code class="xref py py-attr docutils literal notranslate"><span class="pre">robotstxt_url</span></code> check which of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">user_agents</span></code> is
allowed to fetch which of the <code class="xref py py-attr docutils literal notranslate"><span class="pre">urls</span></code>.</p>
<p>All the combinations of <code class="xref py py-attr docutils literal notranslate"><span class="pre">user_agents</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">urls</span></code> will be
checked and the results returned in one DataFrame.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>robotstxt_url</strong> (<em>str</em>) -- The URL of robotx.txt file.</p></li>
<li><p><strong>user_agents</strong> (<em>str</em><em>, </em><em>list</em>) -- One or more user agents.</p></li>
<li><p><strong>urls</strong> (<em>str</em><em>, </em><em>list</em>) -- One or more paths (relative) or URLs (absolute) to check.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>robotstxt_test_df</strong> -- A DataFrame with the test results per user-agent/rule combination.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">robotstxt_test</span><span class="p">(</span>
<span class="gp">... </span>    <span class="s2">&quot;https://facebook.com/robots.txt&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">user_agents</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="s2">&quot;Googlebot&quot;</span><span class="p">,</span> <span class="s2">&quot;Applebot&quot;</span><span class="p">],</span>
<span class="gp">... </span>    <span class="n">urls</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="s2">&quot;/bbc&quot;</span><span class="p">,</span> <span class="s2">&quot;/groups&quot;</span><span class="p">,</span> <span class="s2">&quot;/hashtag/&quot;</span><span class="p">],</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">                      robotstxt_url user_agent   url_path  can_fetch</span>
<span class="go">0   https://facebook.com/robots.txt          *          /      False</span>
<span class="go">1   https://facebook.com/robots.txt          *       /bbc      False</span>
<span class="go">2   https://facebook.com/robots.txt          *    /groups      False</span>
<span class="go">3   https://facebook.com/robots.txt          *  /hashtag/      False</span>
<span class="go">4   https://facebook.com/robots.txt   Applebot          /       True</span>
<span class="go">5   https://facebook.com/robots.txt   Applebot       /bbc       True</span>
<span class="go">6   https://facebook.com/robots.txt   Applebot    /groups       True</span>
<span class="go">7   https://facebook.com/robots.txt   Applebot  /hashtag/      False</span>
<span class="go">8   https://facebook.com/robots.txt  Googlebot          /       True</span>
<span class="go">9   https://facebook.com/robots.txt  Googlebot       /bbc       True</span>
<span class="go">10  https://facebook.com/robots.txt  Googlebot    /groups       True</span>
<span class="go">11  https://facebook.com/robots.txt  Googlebot  /hashtag/      False</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="advertools.robotstxt.robotstxt_to_df">
<span class="sig-name descname"><span class="pre">robotstxt_to_df</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">robotstxt_url</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/advertools/robotstxt.html#robotstxt_to_df"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#advertools.robotstxt.robotstxt_to_df" title="Link to this definition">ÔÉÅ</a></dt>
<dd><p>Download the contents of <code class="docutils literal notranslate"><span class="pre">robotstxt_url</span></code> into a DataFrame</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>robotstxt_url</strong> (<em>str</em>) -- One or more URLs of the robots.txt file(s)</p></li>
<li><p><strong>output_file</strong> (<em>str</em>) -- Optional file path to save the robots.txt files, mainly useful for downloading &gt;
500 files. The files are appended as soon as they are downloaded. Only the &quot;.jl&quot;
extension is supported.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>robotstxt_df</strong> -- A DataFrame containing directives, their content, the URL and time of download</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>pandas.DataFrame</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>You can also use it to download multiple robots files by passing a list of
URLs.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">robotstxt_to_df</span><span class="p">(</span><span class="s2">&quot;https://www.twitter.com/robots.txt&quot;</span><span class="p">)</span>
<span class="go">     directive content                       robotstxt_url                     download_date</span>
<span class="go">0   User-agent           *  https://www.twitter.com/robots.txt      2020-09-27 21:57:23.702814+00:00</span>
<span class="go">1     Disallow           /  https://www.twitter.com/robots.txt      2020-09-27 21:57:23.702814+00:00</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">robotstxt_to_df</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span><span class="s2">&quot;https://www.google.com/robots.txt&quot;</span><span class="p">,</span> <span class="s2">&quot;https://www.twitter.com/robots.txt&quot;</span><span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
<span class="go">       directive                                 content        robotstxt_last_modified                            robotstxt_url                         download_date</span>
<span class="go">0     User-agent                                       *      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">1       Disallow                                 /search      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">2          Allow                           /search/about      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">3          Allow                          /search/static      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">4          Allow                  /search/howsearchworks      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">283   User-agent                     facebookexternalhit      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">284        Allow                                 /imgres      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">285      Sitemap      https://www.google.com/sitemap.xml      2021-01-11 21:00:00+00:00        https://www.google.com/robots.txt      2021-01-16 14:08:50.087985+00:00</span>
<span class="go">286   User-agent                                       *                            NaT       https://www.twitter.com/robots.txt      2021-01-16 14:08:50.468588+00:00</span>
<span class="go">287     Disallow                                       /                            NaT       https://www.twitter.com/robots.txt      2021-01-16 14:08:50.468588+00:00</span>
</pre></div>
</div>
<p>For research purposes and if you want to download more than ~500 files, you
might want to use <code class="docutils literal notranslate"><span class="pre">output_file</span></code> to save results as they are downloaded.
The file extension should be &quot;.jl&quot;, and robots files are appended to that
file as soon as they are downloaded, in case you lose your connection, or
maybe your patience!</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">robotstxt_to_df</span><span class="p">(</span>
<span class="gp">... </span>    <span class="p">[</span>
<span class="gp">... </span>        <span class="s2">&quot;https://example.com/robots.txt&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;https://example.com/robots.txt&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;https://example.com/robots.txt&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">],</span>
<span class="gp">... </span>    <span class="n">output_file</span><span class="o">=</span><span class="s2">&quot;robots_output_file.jl&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>To open the file as a DataFrame:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">robotsfiles_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_json</span><span class="p">(</span><span class="s2">&quot;robots_output_file.jl&quot;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "eliasdabbas/adv_docs_thebe",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="advertools.ad_from_string.html" class="btn btn-neutral float-left" title="Create Ads Using Long Descriptive Text (top-down approach)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="advertools.sitemaps.html" class="btn btn-neutral float-right" title="Download, Parse, and Analyze XML Sitemaps" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, Elias Dabbas.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>