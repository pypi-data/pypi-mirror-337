[stream](https://js.langchain.com/docs/concepts/streaming/): LLM should read this page when looking for ways to implement real-time output streaming in LLM applications, when needing to understand different streaming APIs in LangChain, or when troubleshooting performance issues with streaming implementations. (The page explains streaming in LangChain, covering what data to stream (LLM outputs, workflow progress, custom data), the main streaming APIs (stream() and streamEvents()), how to write custom data to streams, auto-streaming features for chat models, and links to related resources and how-to guides.)

