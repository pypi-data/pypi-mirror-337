# Options regarding the bayesian optimization loop.
# NB: 'd' after a number means the dimensionality of the sampling space,
#     and a number following 'd' means the power of the dimensionality factor.

# General options for the main loop
options:
  # Number of finite initial truth evaluations before starting the learning loop
  n_initial: 3d
  # Maximum number of truth evaluations at initialization. If it is reached before
  # `n_initial` finite points have been found, the run will fail. To avoid that, try
  # decreasing the volume of your prior
  max_initial: 30d1.5
  # Maximum number of truth evaluations before the run stops. This is useful for e.g.
  # restricting the maximum computation resources.
  max_total: 70d1.5
  # Maximum number of sampling points accepted into the GP training set before the run
  # stops. If this limit is frequently saturated, try decreasing the prior volume.
  max_finite:  # default (undefined) = max_total
  # Number of points which are aquired with Kriging believer for every acquisition step.
  # Gets adjusted (with 20% tol.) to match a multiple of the num. of parallel processes.
  n_points_per_acq: d
  # Number of iterations between full GP hyperparameters fits, including several
  # restarts of the optimiser. Pass 'np.inf' or a large number to never refit with restarts.
  fit_full_every: 2d0.5
  # Similar to `fit_full_every`, but with a single optimiser run from the last optimum.
  # Overridden by `fit_full_every` if it applies. Pass np.inf or a large number to never
  # refit from last optimum (hyperparameters kept constant in that iteration).
  fit_simple_every: 1  # every iteration

# The GP regressor used for interpolating the posterior
gpr:
  # Spatial correlation kernel and params, e.g. RBF, Matern, {Matern: {nu: 2.5}}, ...
  kernel: RBF
  # Priors for the output and length scale, in normalised logp units
  output_scale_prior: [1e-2, 1e3]
  length_scale_prior: [1e-3, 1e1]
  # Noise level in logp units; increase for numerically noisy likelihoods
  noise_level: 1e-1
  # Factor used to clip the GPR from above, to avoid overshoots (undefined to disable)
  clip_factor: 1.1
  # Treatment of infinities and large negative values; False for no classifier
  account_for_inf: SVM
  # Difference in standard deviations ('s') for considering a value as -infinity
  inf_threshold: 20s
  # Hyperparameter fitting: optimizer (from scipy) and number of restarts for full fits
  optimizer: fmin_l_bfgs_b
  n_restarts_optimizer: 3d
  # Verbosity (set only if different from the overall verbosity)
  verbose:
  # Shrinkage of the model region, if necessary for speed and robustness
  trust_region_nstd:
  trust_region_factor:
  # [Not interfaced yet] Preprocessors for input and output scales
  # preprocessing_X: Normalize_bounds
  # preprocessing_y: Normalize_y

# The acquisition class, function and their options
gp_acquisition:
  # Acquisition function and its arguments
  acq_func:
    LogExp: {zeta_scaling: 0.85}
  # Verbosity (set only if different from the overall verbosity)
  verbose:
  # Acquisition engine: NORA or BatchOptimizer
  engine: BatchOptimizer
  # Options for the engine (only the relevant ones are kept in the .updated.yaml)
  options_BatchOptimizer:
    proposer:  # default (undefined): a mixture of uniform and centroids
    acq_optimizer: fmin_l_bfgs_b  # scipy optimiser to use
    n_restarts_optimizer: 5d  # number of restarts during hyperparameter fitting
    n_repeats_propose: 10  # number of starting points drawn from the proposer
  options_NORA:
    # nested sampler used for acquisition
    sampler:  # undefined: in order, of available: polychord > ultranest > nessai
    mc_every: 2d  # number of iterations between full NS runs
    nlive_per_training: 3  # number of live points per training sample
    nlive_max: 25d  # cap for the number of live points
    num_repeats: 5d  # number of steps of slice chains (polychord only)
    precision_criterion_target: 0.01  # precision criterion for the NS
    nprior_per_nlive: 10  # number of prior points in the initial sample, times nlive
    max_ncalls:  # maximum number of calls to the GPR model during NS

# Proposer used for drawing the initial training samples before running
# the acquisition loop. One of [reference, prior, uniform].
# Can be specified as dict with args, e.g. {reference: {max_tries: 1000}}
initial_proposer: reference

# Convergence criterion.
# Can be specified as a dict with args, e.g. {CorrectCounter: {abstol: 0.01s}}
convergence_criterion: CorrectCounter

# Cobaya sampler used to generate the final sample from the surrogate model
mc_sampler: mcmc  # default: mcmc with Cobaya defaults

# Produce progress plots (inside the gpry_output dir).
# One can specify options detailing which plots will be made, and in which format, e.g.:
# {timing: True, convergence: True, trace: False, slices: False, format: svg}
# (Adds overhead for very fast likelihoods.)
plots: False

# Function run each iteration after adapting the recently acquired points and
# the computation of the convergence criterion. See docs for implementation.
callback:

# Whether the callback function handles MPI-parallelization internally.
# Otherwise run only by the rank-0 process
callback_is_MPI_aware:

# Change to increase or reduce verbosity. If None, it is handled by Cobaya.
# '3' produces general progress output (default for Cobaya if None),
# and '4' debug-level output
verbose:
