{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "temp_output_dir='./temp'\n",
    "model_dir = temp_output_dir + '/base_pretrained_model'\n",
    "os.makedirs(temp_output_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   140  100   140    0     0    889      0 --:--:-- --:--:-- --:--:--   891\n",
      "100    17  100    17    0     0     19      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 1887M  100 1887M    0     0  79.1M      0  0:00:23  0:00:23 --:--:-- 90.9M\n",
      "Archive:  temp_model.zip\n",
      "warning:  stripped absolute path spec from /\n",
      "mapname:  conversion of  failed\n",
      " extracting: ./temp/base_pretrained_model/vocab.json  \n",
      " extracting: ./temp/base_pretrained_model/merges.txt  \n",
      " extracting: ./temp/base_pretrained_model/config.json  \n",
      " extracting: ./temp/base_pretrained_model/scheduler.pt  \n",
      " extracting: ./temp/base_pretrained_model/pytorch_model.bin  \n",
      " extracting: ./temp/base_pretrained_model/training_args.bin  \n",
      " extracting: ./temp/base_pretrained_model/tokenizer_config.json  \n",
      " extracting: ./temp/base_pretrained_model/special_tokens_map.json  \n"
     ]
    }
   ],
   "source": [
    "!curl -L https://www.dropbox.com/sh/7hpw662xylbmi5o/AAC3nfP4xdGAkf0UkFGzAbrja?dl=1 > temp_model.zip\n",
    "!unzip temp_model.zip -d $model_dir\n",
    "#!rm -rf temp_model.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kramesh3/envs/sdg-env/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from synthtexteval.eval.downstream.coref.minimize_synth import minimize_file\n",
    "\n",
    "synthetic_data_path = \"/data/coref-updated-files/filtered/mimic/princeton_mimic_10ICD_DP_8.csv\"\n",
    "output_path = \"./temp\"\n",
    "sample_size = 100\n",
    "minimize_file(synthetic_data_path, output_path, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kramesh3/envs/sdg-env/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   output_dir - ./temp\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   model_type - longformer\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   base_model_name_or_path - ./temp/base_pretrained_model/\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   train_file - None\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   test_file - /home/kramesh3/synthtexteval/data/coref/test.i2b2.jsonlines\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   predict_file - ./temp/silver.jsonlines\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   predict_file_write - ./temp/silver.pred.jsonlines\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   config_name - allenai/longformer-base-4096\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   tokenizer_name - allenai/longformer-large-4096\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   cache_dir - None\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   max_seq_length - 4000\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   do_train - False\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   do_infer - True\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   do_eval - False\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   do_lower_case - False\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   nonfreeze_params - None\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   learning_rate - 1e-05\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   head_learning_rate - 1e-05\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   dropout_prob - 0.3\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   gradient_accumulation_steps - 1\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   weight_decay - 0.01\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   adam_beta1 - 0.9\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   adam_beta2 - 0.98\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   adam_epsilon - 1e-06\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   num_train_epochs - 3\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   warmup_steps - 5600\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   logging_steps - 100\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   eval_steps - 150\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   save_steps - 1000\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   no_cuda - False\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   overwrite_output_dir - False\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   seed - 42\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   local_rank - -1\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   amp - False\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   fp16_opt_level - O1\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   max_span_length - 30\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   top_lambda - 0.4\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   max_total_seq_len - 4000\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   experiment_name - test-run\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   normalise_loss - True\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   ffnn_size - 3072\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   save_if_best - True\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   batch_size_1 - False\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   tensorboard_dir - ./temp/tb\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   conll_path_for_eval - None\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   n_gpu - 6\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   device - cuda\n",
      "02/26/2025 20:16:40 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   Process rank: -1, device: cuda, n_gpu: 6, distributed training: False, amp training: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CorefArgs(output_dir='./temp', model_type='longformer', base_model_name_or_path='./temp/base_pretrained_model/', train_file=None, test_file='/home/kramesh3/synthtexteval/data/coref/test.i2b2.jsonlines', predict_file='./temp/silver.jsonlines', predict_file_write='./temp/silver.pred.jsonlines', config_name='allenai/longformer-base-4096', tokenizer_name='allenai/longformer-large-4096', cache_dir=None, max_seq_length=4000, do_train=False, do_infer=True, do_eval=False, do_lower_case=False, nonfreeze_params=None, learning_rate=1e-05, head_learning_rate=1e-05, dropout_prob=0.3, gradient_accumulation_steps=1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, num_train_epochs=3, warmup_steps=5600, logging_steps=100, eval_steps=150, save_steps=1000, no_cuda=False, overwrite_output_dir=False, seed=42, local_rank=-1, amp=False, fp16_opt_level='O1', max_span_length=30, top_lambda=0.4, max_total_seq_len=4000, experiment_name='test-run', normalise_loss=True, ffnn_size=3072, save_if_best=True, batch_size_1=False, tensorboard_dir='./temp/tb', conll_path_for_eval=None)\n",
      "args.model_name_or_path ./temp/base_pretrained_model/\n",
      "args CorefArgs(output_dir='./temp', model_type='longformer', base_model_name_or_path='./temp/base_pretrained_model/', train_file=None, test_file='/home/kramesh3/synthtexteval/data/coref/test.i2b2.jsonlines', predict_file='./temp/silver.jsonlines', predict_file_write='./temp/silver.pred.jsonlines', config_name='allenai/longformer-base-4096', tokenizer_name='allenai/longformer-large-4096', cache_dir=None, max_seq_length=4000, do_train=False, do_infer=True, do_eval=False, do_lower_case=False, nonfreeze_params=None, learning_rate=1e-05, head_learning_rate=1e-05, dropout_prob=0.3, gradient_accumulation_steps=1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, num_train_epochs=3, warmup_steps=5600, logging_steps=100, eval_steps=150, save_steps=1000, no_cuda=False, overwrite_output_dir=False, seed=42, local_rank=-1, amp=False, fp16_opt_level='O1', max_span_length=30, top_lambda=0.4, max_total_seq_len=4000, experiment_name='test-run', normalise_loss=True, ffnn_size=3072, save_if_best=True, batch_size_1=False, tensorboard_dir='./temp/tb', conll_path_for_eval=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./temp/base_pretrained_model/ were not used when initializing S2E: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing S2E from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing S2E from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "02/26/2025 20:16:42 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   Training/evaluation parameters CorefArgs(output_dir='./temp', model_type='longformer', base_model_name_or_path='./temp/base_pretrained_model/', train_file=None, test_file='/home/kramesh3/synthtexteval/data/coref/test.i2b2.jsonlines', predict_file='./temp/silver.jsonlines', predict_file_write='./temp/silver.pred.jsonlines', config_name='allenai/longformer-base-4096', tokenizer_name='allenai/longformer-large-4096', cache_dir=None, max_seq_length=4000, do_train=False, do_infer=True, do_eval=False, do_lower_case=False, nonfreeze_params=None, learning_rate=1e-05, head_learning_rate=1e-05, dropout_prob=0.3, gradient_accumulation_steps=1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, num_train_epochs=3, warmup_steps=5600, logging_steps=100, eval_steps=150, save_steps=1000, no_cuda=False, overwrite_output_dir=False, seed=42, local_rank=-1, amp=False, fp16_opt_level='O1', max_span_length=30, top_lambda=0.4, max_total_seq_len=4000, experiment_name='test-run', normalise_loss=True, ffnn_size=3072, save_if_best=True, batch_size_1=False, tensorboard_dir='./temp/tb', conll_path_for_eval=None)\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]Input ids are automatically padded to be a multiple of `config.attention_window`: 512\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.42it/s]\n",
      "/home/kramesh3/.local/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]/data/kramesh3/envs/sdg-env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Iteration: 100%|██████████| 15/15 [00:15<00:00,  1.04s/it]\n",
      "Iteration: 100%|██████████| 15/15 [00:12<00:00,  1.18it/s]\n",
      "Iteration: 100%|██████████| 15/15 [00:12<00:00,  1.17it/s]\n",
      "Epoch: 100%|██████████| 3/3 [00:41<00:00, 13.69s/it]\n",
      "02/26/2025 20:18:08 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -    global_step = 45, average loss = 0.024212146547829938\n",
      "02/26/2025 20:18:08 - INFO - synthtexteval.eval.downstream.coref.run_coref_comparison -   Saving model checkpoint to ./temp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance improvement:\n",
      "loss:-0.124\n",
      "post pruning mention precision:-0.005\n",
      "post pruning mention recall:-0.034\n",
      "post pruning mention f1:-0.008\n",
      "mention precision:0.033\n",
      "mention recall:-0.018\n",
      "mention f1:-0.003\n",
      "precision:0.027\n",
      "recall:-0.015\n",
      "f1:-0.004\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': -0.12437455024890554,\n",
       " 'post pruning mention precision': -0.004763036246597824,\n",
       " 'post pruning mention recall': -0.034162212409946546,\n",
       " 'post pruning mention f1': -0.008360427310729168,\n",
       " 'mention precision': 0.033223849472772016,\n",
       " 'mention recall': -0.0175071655434193,\n",
       " 'mention f1': -0.0027451763873648227,\n",
       " 'precision': 0.02730579682200529,\n",
       " 'recall': -0.014653817917044343,\n",
       " 'f1': -0.004160874840206663}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from synthtexteval.eval.downstream.coref.run_coref_comparison import coref_train\n",
    "from synthtexteval.eval.downstream.coref.arguments import set_default_coref_args\n",
    "\n",
    "args = set_default_coref_args()\n",
    "print(args)\n",
    "coref_train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
