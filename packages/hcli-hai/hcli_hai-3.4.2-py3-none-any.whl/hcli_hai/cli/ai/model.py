# max_token controls and caps off the response size.

models = {
    "claude-3-haiku-20240307": {
                   'model' : "claude-3-haiku-20240307",
                   'max_tokens' : 2048
               },
    "claude-3-5-haiku-20241022": {
                   'model' : "claude-3-5-haiku-20241022",
                   'max_tokens' : 8192
               },
    "claude-3-5-haiku-latest": {
                   'model' : "claude-3-5-haiku-latest",
                   'max_tokens' : 8192
               },
    "claude-3-5-sonnet-latest": {
                   'model' : "claude-3-5-sonnet-20240620",
                   'max_tokens' : 8192
               },
    "claude-3-5-sonnet-20241022": {
                   'model' : "claude-3-5-sonnet-20241022",
                   'max_tokens' : 8192
               },
    "claude-3-5-sonnet-20240620": {
                   'model' : "claude-3-5-sonnet-20240620",
                   'max_tokens' : 8192
               },
    "claude-3-opus-latest": {
                   'model' : "claude-3-opus-latest",
                   'max_tokens' : 4096
               },
    "claude-3-opus-20240229": {
                   'model' : "claude-3-opus-20240229",
                   'max_tokens' : 4096
               },
    "claude-3-7-sonnet-20250219": {
                   'model' : "claude-3-7-sonnet-20250219",
                   'max_tokens' : 8192
               },
    "claude-3-7-sonnet-latest": {
                   'model' : "claude-3-7-sonnet-latest",
                   'max_tokens' : 8192
               },
    "claude-3-sonnet-20240229": {
                   'model' : "claude-3-sonnet-20240229",
                   'max_tokens' : 4096
               }
}
