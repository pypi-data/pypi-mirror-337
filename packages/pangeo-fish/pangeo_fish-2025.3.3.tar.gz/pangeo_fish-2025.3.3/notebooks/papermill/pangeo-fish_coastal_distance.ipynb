{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# **Example Usage of Pangeo-Fish Software**\n",
    "\n",
    "\n",
    "**Overview:**\n",
    "This Jupyter notebook demonstrates the usage of the Pangeo-Fish software, a tool designed for analyzing biologging data in reference to Earth Observation (EO) data.\n",
    "\n",
    "The biologging data consist of Data Storage Tag (DST), along with release and recapture time and location of the species in question. Both biologging data and the reference EO data are accessible with https and the access methods are incorporated in this notebook.   \n",
    "\n",
    "**Purpose:**\n",
    "By executing this notebook, users will learn how to set up a workflow for utilizing the Pangeo-Fish software. The workflow consists of 9 steps which are described below:\n",
    "\n",
    "1. **Configure the Notebook:** Prepare the notebook environment for analysis.\n",
    "2. **Compare Reference Model with DST Information:** Analyze and compare data from the reference model with information from the biologging data of the species in question. \n",
    "3. **Regrid the Grid from Reference Model Grid to Healpix Grid:** Transform the grid from the reference model to the Healpix grid for further analysis.\n",
    "4. **Construct Emission Matrix:** Create an emission matrix based on the transformed grid.\n",
    "5. **Replace emission for flagged tags:** If the tags are flagged for warm water, then it use the detection file associated and change the flagged timestamps.\n",
    "6. **Combine and Normalize Emission Matrix:** Merge the emission matrix and normalize it for further processing.\n",
    "7. **Estimate Model Parameters:** Determine the parameters of the model based on the normalized emission matrix.\n",
    "8. **Compute State Probabilities and Tracks:** Calculate the probability distribution of the species in question and compute the tracks.\n",
    "9. **Visualization:** Visualize the results of the analysis for interpretation and insight.\n",
    "\n",
    "Throughout this notebook, users will gain practical experience in setting up and executing a workflow using Pangeo-Fish, enabling them to apply similar methodologies to their own biologging data analysis tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 1. **Configure the Notebook:** Prepare the notebook environment for analysis.\n",
    "\n",
    "In this step, we sets up the notebook environment for analysis. It includes installing necessary packages, importing required libraries, setting up parameters, and configuring the cluster for distributed computing. It also retrieves the tag data needed for analysis.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "2",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "!pip install ../../git/pangeo-fish/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "!pip install copernicusmarine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules.\n",
    "import hvplot.xarray\n",
    "import intake\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pint_xarray import unit_registry as ureg\n",
    "\n",
    "from pangeo_fish.io import open_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Set up execution parameters for the analysis.\n",
    "#\n",
    "# Note: This cell is tagged as parameters, allowing automatic updates when configuring with papermil.\n",
    "\n",
    "# tag_name corresponds to the name of the biologging tag name (DST identification number),\n",
    "# which is also a path for storing all the information for the specific fish tagged with tag_name.\n",
    "# tag_name = \"AD_A11849\"\n",
    "# tag_name = \"SV_A11957\"\n",
    "\n",
    "\n",
    "tag_list = [\n",
    "    \"NO_A12710\",\n",
    "    \"CB_A11036\",\n",
    "    \"LT_A11385\",\n",
    "    \"SQ_A10684\",\n",
    "    \"AD_A11177\",\n",
    "    \"PB_A12063\",\n",
    "    \"NO_A12742\",\n",
    "    \"DK_A10642\",\n",
    "    \"CB_A11071\",\n",
    "]\n",
    "tag_name = tag_list[8]\n",
    "tag_name = \"LT_A11338\"\n",
    "\n",
    "cloud_root = \"s3://gfts-ifremer/tags/bargip\"\n",
    "\n",
    "# tag_root specifies the root URL for tag data used for this computation.\n",
    "tag_root = f\"{cloud_root}/cleaned\"\n",
    "\n",
    "# catalog_url specifies the URL for the catalog for reference data used.\n",
    "catalog_url = \"s3://gfts-ifremer/copernicus_catalogs/master.yml\"\n",
    "\n",
    "# scratch_root specifies the root directory for storing output files.\n",
    "scratch_root = f\"{cloud_root}/tracks\"\n",
    "\n",
    "\n",
    "# storage_options specifies options for the filesystem storing output files.\n",
    "storage_options = {\n",
    "    \"anon\": False,\n",
    "    # 'profile' : \"gfts\",\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "        \"region_name\": \"gra\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# if you are using local file system, activate following two lines\n",
    "folder_name = \"../toto\"\n",
    "storage_options = None\n",
    "scratch_root = f\"/home/jovyan/notebooks/papermill/{folder_name}\"\n",
    "\n",
    "# Default chunk value for time dimension.  This values depends on the configuration of your dask cluster.\n",
    "chunk_time = 24\n",
    "\n",
    "#\n",
    "# Parameters for step 2. **Compare Reference Model with DST Information:**\n",
    "#\n",
    "# bbox, bounding box, defines the latitude and longitude range for the analysis area.\n",
    "bbox = {\"latitude\": [40, 56], \"longitude\": [-13, 5]}\n",
    "\n",
    "# relative_depth_threshold defines the acceptable fish depth relative to the maximum tag depth.\n",
    "# It determines whether the fish can be considered to be in a certain location based on depth.\n",
    "relative_depth_threshold = 0.8\n",
    "\n",
    "#\n",
    "# Parameters for step 3. **Regrid the Grid from Reference Model Grid to Healpix Grid:**\n",
    "#\n",
    "# Distance filepath is the path to the coastal distance file.\n",
    "distance_filepath = \"s3://gfts-ifremer/tags/distance2coast.zarr\"\n",
    "\n",
    "# distance_scale_factor scales the squared distance in the exponential decay function.\n",
    "distance_scale_factor = 0.01\n",
    "\n",
    "# nside defines the resolution of the healpix grid used for regridding.\n",
    "nside = 4096  # *2\n",
    "\n",
    "# rot defines the rotation angles for the healpix grid.\n",
    "rot = {\"lat\": 0, \"lon\": 30}\n",
    "\n",
    "# min_vertices sets the minimum number of vertices for a valid transcription for regridding.\n",
    "min_vertices = 1\n",
    "\n",
    "#\n",
    "# Parameters for step 4. **Construct Emission Matrix:**\n",
    "#\n",
    "# differences_std sets the standard deviation for scipy.stats.norm.pdf.\n",
    "# It expresses the estimated certainty of the field of difference.\n",
    "differences_std = 0.75\n",
    "\n",
    "# recapture_std sets the covariance for recapture event.\n",
    "# It shows the certainty of the final recapture area if it is known.\n",
    "recapture_std = 1e-2\n",
    "\n",
    "# earth_radius defines the radius of the Earth used for distance calculations.\n",
    "earth_radius = ureg.Quantity(6371, \"km\")\n",
    "\n",
    "# maximum_speed sets the maximum allowable speed for the tagged fish.\n",
    "maximum_speed = ureg.Quantity(60, \"km / day\")\n",
    "\n",
    "# adjustment_factor adjusts parameters for a more fuzzy search.\n",
    "# It will factor the allowed maximum displacement of the fish.\n",
    "adjustment_factor = 5\n",
    "\n",
    "# truncate sets the truncating factor for computed maximum allowed sigma for convolution process.\n",
    "truncate = 4\n",
    "\n",
    "#\n",
    "# Parameters for step 5. **Compute Additional Emission Probability Matrix:**\n",
    "#\n",
    "\n",
    "\n",
    "# buffer_size sets the size of the powerplant warm plume.\n",
    "buffer_size = ureg.Quantity(1000, \"m\")\n",
    "# powerplant_flag is a boolean that states if the fish has swam in warm plume\n",
    "\n",
    "\n",
    "#\n",
    "# Parameters for step 7. **Estimate Model Parameters:**\n",
    "#\n",
    "# tolerance sets the tolerance level for optimised parameter serarch computation.\n",
    "tolerance = 1e-3\n",
    "\n",
    "#\n",
    "# Parameters for step 8. **Compute State Probabilities and Tracks:**\n",
    "#\n",
    "# track_modes defines the modes for track calculation.\n",
    "track_modes = [\"mean\", \"mode\"]\n",
    "\n",
    "# additional_track_quantities sets quantities to compute for tracks using moving pandas.\n",
    "additional_track_quantities = [\"speed\", \"distance\"]\n",
    "\n",
    "\n",
    "#\n",
    "# Parameters for step 9. **Visualization:**\n",
    "#\n",
    "# time_step defines for each time_step value we visualize state and emission matrix.\n",
    "time_step = 3\n",
    "\n",
    "\n",
    "# Define target root directories for storing analysis results.\n",
    "target_root = f\"{scratch_root}/{tag_name}\"\n",
    "\n",
    "# Defines default chunk size for optimisation.\n",
    "default_chunk = {\"time\": chunk_time, \"lat\": -1, \"lon\": -1}\n",
    "default_chunk_xy = {\"time\": chunk_time, \"x\": -1, \"y\": -1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target root directories for storing analysis results.\n",
    "target_root = f\"{scratch_root}/{tag_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tag_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_plume = pd.read_csv(\n",
    "    \"s3://gfts-ifremer/tags/bargip/bar_flag_warm_plume.txt\", sep=\"\\t\"\n",
    ")\n",
    "warm_list = list(warm_plume[warm_plume[\"warm_plume\"] == True][\"tag_name\"])\n",
    "\n",
    "if tag_name in warm_list:\n",
    "    powerplant_flag = True\n",
    "else:\n",
    "    powerplant_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if powerplant_flag:\n",
    "    detection_file = f\"{tag_root}/{tag_name}/detection.csv\"\n",
    "    powerplant_file = f\"{cloud_root}/nuclear_plant_loc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up a local cluster for distributed computing.\n",
    "from distributed import LocalCluster\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = cluster.get_client()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open and retrieve the tag data required for the analysis\n",
    "tag = open_tag(tag_root, tag_name)\n",
    "tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2. **Compare Reference Model with DST Tag Information:** Analyze and compare data from the reference model with information from the biologging data of the species in question. \n",
    "\n",
    "In this step, we compare the reference model data with Data Storage Tag information.\n",
    "The process involves reading and cleaning the reference model, aligning time, converting depth units, subtracting tag data from the model, and saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import intake\n",
    "\n",
    "from pangeo_fish.cf import bounds_to_bins\n",
    "from pangeo_fish.diff import diff_z\n",
    "from pangeo_fish.io import open_copernicus_catalog\n",
    "from pangeo_fish.tags import adapt_model_time, reshape_by_bins, to_time_slice\n",
    "\n",
    "# Drop data outside the reference interval\n",
    "time_slice = to_time_slice(tag[\"tagging_events/time\"])\n",
    "time = tag[\"dst\"].ds.time\n",
    "cond = (time <= time_slice.stop) & (time >= time_slice.start)\n",
    "\n",
    "tag_log = tag[\"dst\"].ds.where(cond, drop=True)\n",
    "\n",
    "min_ = tag_log.time[0]\n",
    "max_ = tag_log.time[-1]\n",
    "\n",
    "time_slice = slice(min_.data, max_.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_copernicus_zarr(product_id=\"IBI_MULTIYEAR_PHY_005_002\"):\n",
    "    master_cat = intake.open_catalog(catalog_url)\n",
    "    if product_id == \"IBI_MULTIYEAR_PHY_005_002\":\n",
    "\n",
    "        # Open necessary datasets\n",
    "        sub_cat = master_cat[product_id]\n",
    "        thetao = sub_cat[\"cmems_mod_ibi_phy_my_0.083deg-3D_P1D-m\"](\n",
    "            chunk=\"time\"\n",
    "        ).to_dask()[[\"thetao\"]]\n",
    "        zos = (\n",
    "            sub_cat[\"cmems_mod_ibi_phy_my_0.083deg-3D_P1D-m\"](chunk=\"time\")\n",
    "            .to_dask()\n",
    "            .zos\n",
    "        )\n",
    "        deptho = sub_cat[\"cmems_mod_ibi_phy_my_0.083deg-3D_static\"].to_dask().deptho\n",
    "\n",
    "    # Assign latitude and longitude from thetao to deptho to shift in positions\n",
    "    deptho[\"latitude\"] = thetao[\"latitude\"]\n",
    "    deptho[\"longitude\"] = thetao[\"longitude\"]\n",
    "\n",
    "    # Create mask for deptho\n",
    "    mask = deptho.isnull()\n",
    "\n",
    "    # Merge datasets and assign relevant variables\n",
    "    ds = (\n",
    "        thetao.rename({\"thetao\": \"TEMP\"}).assign(\n",
    "            {\n",
    "                \"XE\": zos,\n",
    "                \"H0\": deptho,\n",
    "                \"mask\": mask,\n",
    "            }\n",
    "        )\n",
    "    ).rename({\"latitude\": \"lat\", \"longitude\": \"lon\", \"elevation\": \"depth\"})\n",
    "\n",
    "    # Ensure depth is positive\n",
    "    ds[\"depth\"] = abs(ds[\"depth\"])\n",
    "\n",
    "    # Rearrange depth coordinates and assign dynamic depth and bathymetry\n",
    "    ds = (\n",
    "        ds.isel(depth=slice(None, None, -1))\n",
    "        .assign(\n",
    "            {\n",
    "                \"dynamic_depth\": lambda ds: (ds[\"depth\"] + ds[\"XE\"]).assign_attrs(\n",
    "                    {\"units\": \"m\", \"positive\": \"down\"}\n",
    "                ),\n",
    "                \"dynamic_bathymetry\": lambda ds: (ds[\"H0\"] + ds[\"XE\"]).assign_attrs(\n",
    "                    {\"units\": \"m\", \"positive\": \"down\"}\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "        .pipe(broadcast_variables, {\"lat\": \"latitude\", \"lon\": \"longitude\"})\n",
    "    )\n",
    "    # print(uris_by_key)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "import cmocean\n",
    "import hvplot.xarray\n",
    "\n",
    "from pangeo_fish.io import save_html_hvplot\n",
    "\n",
    "plot = (\n",
    "    (-tag[\"dst\"].pressure).hvplot(width=1000, height=500, color=\"blue\")\n",
    "    * (-tag_log).hvplot.scatter(\n",
    "        x=\"time\", y=\"pressure\", color=\"red\", size=5, width=1000, height=500\n",
    "    )\n",
    "    * (\n",
    "        (tag[\"dst\"].temperature).hvplot(width=1000, height=500, color=\"blue\")\n",
    "        * (tag_log).hvplot.scatter(\n",
    "            x=\"time\", y=\"temperature\", color=\"red\", size=5, width=1000, height=500\n",
    "        )\n",
    "    )\n",
    ")\n",
    "filepath = f\"{target_root}/tags.html\"\n",
    "\n",
    "save_html_hvplot(plot, filepath, storage_options)\n",
    "\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pangeo_fish.io import broadcast_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_copernicus_zarr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subset the reference_model by\n",
    "# - align model time with the time of tag_log, also\n",
    "# - drop data for depth later that are unlikely due to the observed pressure from tag_log\n",
    "# - defined latitude and longitude of bbox.\n",
    "#\n",
    "reference_model = (\n",
    "    model.sel(time=adapt_model_time(time_slice))\n",
    "    .sel(lat=slice(*bbox[\"latitude\"]), lon=slice(*bbox[\"longitude\"]))\n",
    "    .pipe(\n",
    "        lambda ds: ds.sel(\n",
    "            depth=slice(None, (tag_log[\"pressure\"].max() - ds[\"XE\"].min()).compute())\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshape the tag log, so that it bins to the time step of reference_model\n",
    "reshaped_tag = reshape_by_bins(\n",
    "    tag_log,\n",
    "    dim=\"time\",\n",
    "    bins=(\n",
    "        reference_model.cf.add_bounds([\"time\"], output_dim=\"bounds\")\n",
    "        .pipe(bounds_to_bins, bounds_dim=\"bounds\")\n",
    "        .get(\"time_bins\")\n",
    "    ),\n",
    "    bin_dim=\"bincount\",\n",
    "    other_dim=\"obs\",\n",
    ").chunk({\"time\": chunk_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Subtract the time_bined tag_log from the reference_model.\n",
    "# Here, for each time_bin, each observed value are compared with the correspoindng depth of reference_model using diff_z function.\n",
    "#\n",
    "\n",
    "diff = (\n",
    "    diff_z(\n",
    "        reference_model.chunk(dict(depth=-1)),\n",
    "        reshaped_tag,\n",
    "        depth_threshold=relative_depth_threshold,\n",
    "    )\n",
    "    .assign_attrs({\"tag_id\": tag_name})\n",
    "    .assign(\n",
    "        {\n",
    "            \"H0\": reference_model[\"H0\"],\n",
    "            \"ocean_mask\": reference_model[\"H0\"].notnull(),\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "# Persist the diff data\n",
    "diff = diff.chunk(default_chunk).persist()\n",
    "# diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Verify the data\n",
    "# diff[\"diff\"].count([\"lat\",\"lon\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_lat = diff[\"lat\"]\n",
    "target_lon = diff[\"lon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save snapshot to disk\n",
    "diff.to_zarr(f\"{target_root}/diff.zarr\", mode=\"w\", storage_options=storage_options)\n",
    "\n",
    "# Cleanup\n",
    "del tag_log, model, reference_model, reshaped_tag, diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 3. **Regrid the Grid from Reference Model Grid to Healpix Grid:** Transform the grid from the reference model to the Healpix grid for further analysis.\n",
    "\n",
    "In this step, we regrid the data from the reference model grid to a Healpix grid. This process involves defining the Healpix grid, creating the target grid, computing interpolation weights, performing the regridding, and saving the regridded data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import s3fs\n",
    "from xhealpixify import HealpyGridInfo, HealpyRegridder\n",
    "\n",
    "from pangeo_fish.grid import center_longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Open the diff data and performs cleaning operations to prepare it for regridding.\n",
    "\n",
    "ds = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/diff.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "    .pipe(lambda ds: ds.merge(ds[[\"latitude\", \"longitude\"]].compute()))\n",
    "    .swap_dims({\"lat\": \"yi\", \"lon\": \"xi\"})\n",
    ")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(\n",
    "    anon=False,\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"https://s3.gra.perf.cloud.ovh.net\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_distance = xr.open_zarr(distance_filepath).sel(\n",
    "    lat=slice(56, 40), lon=slice(-13, 5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_distance = coastal_distance.sortby(\"lat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_distance = coastal_distance.interp(\n",
    "    lat=target_lat, lon=target_lon, method=\"linear\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_distance[\"dist\"] = 1 + np.exp(\n",
    "    -(coastal_distance.dist * coastal_distance.dist) * distance_scale_factor\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33",
   "metadata": {},
   "source": [
    "(coastal_distance).dist.hvplot(geo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_distance = coastal_distance.swap_dims({\"lat\": \"yi\", \"lon\": \"xi\"}).drop_vars(\n",
    "    [\"lat\", \"lon\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Define the target Healpix grid information\n",
    "grid = HealpyGridInfo(level=int(np.log2(nside)), rot=rot)\n",
    "target_grid = grid.target_grid(ds).pipe(center_longitude, 0)\n",
    "target_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute the interpolation weights for regridding the diff data\n",
    "regridder = HealpyRegridder(\n",
    "    ds[[\"longitude\", \"latitude\", \"ocean_mask\"]],\n",
    "    target_grid,\n",
    "    method=\"bilinear\",\n",
    "    interpolation_kwargs={\"mask\": \"ocean_mask\", \"min_vertices\": min_vertices},\n",
    ")\n",
    "regridder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Perform the regridding operation using the computed interpolation weights.\n",
    "regridded = regridder.regrid_ds(ds)\n",
    "regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "regridded_coastal = regridder.regrid_ds(coastal_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Reshape the regridded data to 2D\n",
    "reshaped = grid.to_2d(regridded).pipe(center_longitude, 0)\n",
    "reshaped = reshaped.persist()\n",
    "reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_coastal = grid.to_2d(regridded_coastal).pipe(center_longitude, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell verifies the regridded data by plotting the count of non-NaN values.\n",
    "# reshaped[\"diff\"].count([\"x\", \"y\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "coastal_chunk = {\"x\": default_chunk_xy[\"x\"], \"y\": default_chunk_xy[\"y\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped[\"diff\"].isel(time=0).hvplot.quadmesh(\n",
    "    title=\"Carte des différences avant l'ajout de l'incertitude\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    cmap=\"cool\",\n",
    "    coastline=\"10m\",\n",
    "    xlim=bbox[\"longitude\"],\n",
    "    ylim=bbox[\"latitude\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped[\"diff\"] = reshaped[\"diff\"] / reshaped_coastal[\"dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped[\"diff\"].isel(time=0).hvplot.quadmesh(\n",
    "    title=\"Carte des différences après l'ajout de l'incertitude\",\n",
    "    x=\"longitude\",\n",
    "    y=\"latitude\",\n",
    "    cmap=\"cool\",\n",
    "    coastline=\"10m\",\n",
    "    xlim=bbox[\"longitude\"],\n",
    "    ylim=bbox[\"latitude\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This cell saves the regridded data to Zarr format, then cleans up unnecessary variables to free up memory after the regridding process.\n",
    "reshaped.chunk(default_chunk_xy).to_zarr(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    compute=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "\n",
    "reshaped_coastal.chunk(coastal_chunk).to_zarr(\n",
    "    f\"{target_root}/coastal.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    compute=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "# Cleanup unnecessary variables to free up memory\n",
    "del ds, grid, target_grid, regridder, regridded, reshaped, reshaped_coastal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 4. **Construct Emission Matrix:** Create an emission matrix based on the transformed grid.\n",
    "\n",
    "In this step, we construct the emission probability matrix based on the differences between the observed tag temperature and the reference sea temperature computed in Workflow 2 and regridded in Workflow 3. The emission probability matrix represents the likelihood of observing a specific temperature difference given the model parameters and configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from toolz.dicttoolz import valfilter\n",
    "\n",
    "from pangeo_fish.distributions import create_covariances, normal_at\n",
    "from pangeo_fish.pdf import normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Open the regridded diff data\n",
    "differences = xr.open_dataset(\n",
    "    f\"{target_root}/diff-regridded.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    storage_options=storage_options,\n",
    ").pipe(lambda ds: ds.merge(ds[[\"latitude\", \"longitude\"]].compute()))\n",
    "differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute initial and final position\n",
    "grid = differences[[\"latitude\", \"longitude\"]].compute()\n",
    "\n",
    "initial_position = tag[\"tagging_events\"].ds.sel(event_name=\"release\")\n",
    "cov = create_covariances(1e-6, coord_names=[\"latitude\", \"longitude\"])\n",
    "initial_probability = normal_at(\n",
    "    grid, pos=initial_position, cov=cov, normalize=True, axes=[\"latitude\", \"longitude\"]\n",
    ")\n",
    "\n",
    "final_position = tag[\"tagging_events\"].ds.sel(event_name=\"fish_death\")\n",
    "if final_position[[\"longitude\", \"latitude\"]].to_dataarray().isnull().all():\n",
    "    final_probability = None\n",
    "else:\n",
    "    cov = create_covariances(recapture_std**2, coord_names=[\"latitude\", \"longitude\"])\n",
    "    final_probability = normal_at(\n",
    "        grid,\n",
    "        pos=final_position,\n",
    "        cov=cov,\n",
    "        normalize=True,\n",
    "        axes=[\"latitude\", \"longitude\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# compute emission probability matrix\n",
    "\n",
    "emission_pdf = (\n",
    "    normal(differences[\"diff\"], mean=0, std=differences_std, dims=[\"y\", \"x\"])\n",
    "    .to_dataset(name=\"pdf\")\n",
    "    .assign(\n",
    "        valfilter(\n",
    "            lambda x: x is not None,\n",
    "            {\n",
    "                \"initial\": initial_probability,\n",
    "                \"final\": final_probability,\n",
    "                \"mask\": differences[\"ocean_mask\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    .assign_attrs(differences.attrs)  # | {\"max_sigma\": max_sigma})\n",
    ")\n",
    "\n",
    "emission_pdf = emission_pdf.chunk(default_chunk_xy).persist()\n",
    "emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data\n",
    "# emission_pdf[\"pdf\"].count([\"x\", \"y\"]).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell saves the emission data to Zarr format, then cleans up unnecessary variables to free up memory.\n",
    "\n",
    "emission_pdf.to_zarr(\n",
    "    f\"{target_root}/emission.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "\n",
    "del differences, grid, initial_probability, final_probability, emission_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## 5. **Replace emission for the tags with warm spikes detected**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Import necessary libraries and open data and perform initial setup\n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "\n",
    "from pangeo_fish import acoustic, utils\n",
    "from pangeo_fish.heat import heat_regulation, powerpalnt_emission_map\n",
    "\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/emission.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},  # \"x\": -1, \"y\": -1},\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "powerplant_flag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if powerplant_flag:\n",
    "    # Loading detections, formatting and reducing observation window\n",
    "    detections = pd.read_csv(detection_file).set_index(\"time\").to_xarray()\n",
    "    detections[\"time\"] = detections[\"time\"].astype(\"datetime64\")\n",
    "    detections = detections.sel(\n",
    "        time=emission[\"time\"]\n",
    "    )  # Narrowing the data to the observed days only\n",
    "\n",
    "    pp_map = (\n",
    "        pd.read_csv(powerplant_file, sep=\";\").drop(\"Country\", axis=1).to_xarray()\n",
    "    )  # Loading powerplant locations data\n",
    "\n",
    "    # Combining and replacing the emission map at the given timestamps for the days where warm plume are detected\n",
    "    combined_masks = powerpalnt_emission_map(pp_map, emission, buffer_size, rot)\n",
    "    emission = heat_regulation(emission, detections, combined_masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 6. **Combine and Normalize Emission Matrix:** Merge the emission matrix and normalize it for further processing.\n",
    "\n",
    "In this step, we combine the emission probability matrix constructed in Workflow 4 and 5 then normalize it to ensure that the probabilities sum up to one. This step prepares the combined emission matrix for further analysis and interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import hvplot.xarray\n",
    "\n",
    "from pangeo_fish.pdf import combine_emission_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open and combine the emission probability matrix\n",
    "\n",
    "combined = (\n",
    "    emission.pipe(combine_emission_pdf)\n",
    "    .chunk(default_chunk_xy)\n",
    "    .persist()  # convert to comment if the emission matrix does *not* fit in memory\n",
    ")\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify the data and visualize the sum of probabilities\n",
    "# combined[\"pdf\"].sum([\"x\", \"y\"]).hvplot(width=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the combined and normalized emission matrix\n",
    "combined.to_zarr(\n",
    "    f\"{target_root}/combined.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "del combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 7. **Estimate Model Parameters:** Determine the parameters of the model based on the normalized emission matrix.\n",
    "\n",
    "This step first estimates maxixmum allowed value of  model parameter 'sigma' max_sigma.  Then we\n",
    "create an optimizer with an expected parameter range, fitting the model to the normalized emission matrix.  \n",
    "The resulting optimized parameters is saved to a json file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for data analysis.\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from pangeo_fish.hmm.estimator import EagerScoreEstimator\n",
    "from pangeo_fish.hmm.optimize import EagerBoundsSearch\n",
    "from pangeo_fish.utils import temporal_resolution\n",
    "\n",
    "# Open the data\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/combined.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ")\n",
    "emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute maximum displacement for each reference model time step\n",
    "# and estimate maximum sigma value for limiting the optimisation step\n",
    "\n",
    "earth_radius_ = xr.DataArray(earth_radius, dims=None)\n",
    "\n",
    "timedelta = temporal_resolution(emission[\"time\"]).pint.quantify().pint.to(\"h\")\n",
    "grid_resolution = earth_radius_ * emission[\"resolution\"].pint.quantify()\n",
    "\n",
    "maximum_speed_ = xr.DataArray(maximum_speed, dims=None).pint.to(\"km / h\")\n",
    "max_grid_displacement = maximum_speed_ * timedelta * adjustment_factor / grid_resolution\n",
    "max_sigma = max_grid_displacement.pint.to(\"dimensionless\").pint.magnitude / truncate\n",
    "emission.attrs[\"max_sigma\"] = max_sigma\n",
    "max_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create and configure estimator and optimizer\n",
    "emission = (\n",
    "    emission.compute()\n",
    ")  # Convert to comment if the emission matrix does *not* fit in memory\n",
    "estimator = EagerScoreEstimator()\n",
    "optimizer = EagerBoundsSearch(\n",
    "    estimator,\n",
    "    (1e-4, emission.attrs[\"max_sigma\"]),\n",
    "    optimizer_kwargs={\"disp\": 3, \"xtol\": tolerance},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Fit the model parameter to the data\n",
    "optimized = optimizer.fit(emission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized parameters\n",
    "params = optimized.to_dict()\n",
    "pd.DataFrame.from_dict(params, orient=\"index\").to_json(\n",
    "    f\"{target_root}/parameters.json\", storage_options=storage_options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del optimized, emission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## 8. **Compute State Probabilities and Tracks:** Calculate the probability distribution of the species in question and compute the tracks.\n",
    "\n",
    "This step involves predicting state probabilities using the optimised parameter sigma computed in the last step together with normalized emission matrix.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and modules for data analysis.\n",
    "import hvplot.xarray\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from pangeo_fish.hmm.estimator import EagerScoreEstimator\n",
    "from pangeo_fish.io import save_trajectories\n",
    "\n",
    "# Recreate the Estimator\n",
    "params = pd.read_json(\n",
    "    f\"{target_root}/parameters.json\", storage_options=storage_options\n",
    ").to_dict()[0]\n",
    "optimized = EagerScoreEstimator(**params)\n",
    "optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load the Data\n",
    "emission = xr.open_dataset(\n",
    "    f\"{target_root}/combined.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks=default_chunk_xy,\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ").compute()\n",
    "\n",
    "# Predict the State Probabilities\n",
    "\n",
    "states = optimized.predict_proba(emission)\n",
    "states = states.to_dataset().chunk(default_chunk_xy).persist()\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the data and visualize the sum of probabilities\n",
    "# states.sum([\"x\", \"y\"]).hvplot() +states.count([\"x\", \"y\"]).hvplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save probability distirbution, state matrix.\n",
    "states.chunk(default_chunk_xy).to_zarr(\n",
    "    f\"{target_root}/states.zarr\",\n",
    "    mode=\"w\",\n",
    "    consolidated=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# decode tracks\n",
    "\n",
    "trajectories = optimized.decode(\n",
    "    emission,\n",
    "    states.fillna(0),\n",
    "    mode=track_modes,\n",
    "    progress=False,\n",
    "    additional_quantities=additional_track_quantities,\n",
    ")\n",
    "trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trajectories.\n",
    "# Here we can chose format parquet for loading files from 'R'\n",
    "# or chose to  format 'geoparquet' for further analysis of tracks using\n",
    "# geopands.\n",
    "\n",
    "save_trajectories(trajectories, target_root, storage_options, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "del optimized, emission, states, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## 9. **Visualization:** Visualize the results of the analysis for interpretation and insight.\n",
    "\n",
    "\n",
    "In this step, we visualize various aspects of the analysis results to gain insights and interpret the model outcomes. We plot the emission matrix, which represents the likelihood of observing a specific temperature difference given the model parameters and configurations. Additionally, we visualize the state probabilities, showing the likelihood of the system being in different states at each time step. We also plot each of the tracks of the tagged fish, displaying their movement patterns over time. Finally, we create a movie that combines the emission matrix and state probabilities to provide a comprehensive visualization of the analysis results.\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "81",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import holoviews as hv\n",
    "import hvplot.xarray\n",
    "import cmocean\n",
    "import xmovie\n",
    "from pangeo_fish import visualization\n",
    "from pangeo_fish.io import read_trajectories, save_html_hvplot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82",
   "metadata": {},
   "source": [
    "# load trajectories \n",
    "trajectories = read_trajectories( track_modes,target_root,storage_options, format=\"parquet\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83",
   "metadata": {},
   "source": [
    "# Plot trajectories and plot.  \n",
    "\n",
    "plots = [\n",
    "    traj.hvplot(c=\"speed\", tiles=\"CartoLight\", title=traj.id, cmap=\"cmo.speed\"\n",
    "#                ,xlim=bbox['longitude'],        ylim=bbox['latitude']\n",
    "                ,width=300,height=300)\n",
    "    for traj in trajectories.trajectories\n",
    "]\n",
    "plot=hv.Layout(plots).cols(2)\n",
    "\n",
    "filepath = f\"{target_root}/trajectories.html\"\n",
    "save_html_hvplot(plot, filepath, storage_options)\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84",
   "metadata": {},
   "source": [
    "xr.open_dataset(\n",
    "    f\"{target_root}/combined.zarr\",\n",
    "    engine=\"zarr\",\n",
    "    chunks={},\n",
    "    inline_array=True,\n",
    "    storage_options=storage_options,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85",
   "metadata": {},
   "source": [
    "# load files for plotting\n",
    "\n",
    "emission = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/combined.zarr\",\n",
    "        engine=\"zarr\",\n",
    "        chunks={},\n",
    "        inline_array=True,\n",
    "        storage_options=storage_options,\n",
    "    )\n",
    "    .rename_vars({\"pdf\": \"emission\"})).drop_vars([\"initial\"])\n",
    "\n",
    "if \"final\" in emission.data_vars:\n",
    "    emission = emission.drop_vars([\"final\"])\n",
    "    \n",
    "states = (\n",
    "    xr.open_dataset(\n",
    "        f\"{target_root}/states.zarr\", \n",
    "        engine=\"zarr\", \n",
    "        chunks={}, \n",
    "        inline_array=True,\n",
    "        storage_options=storage_options,\n",
    "    ).where(emission[\"mask\"])\n",
    ")\n",
    "data = xr.merge([states, emission.drop_vars([\"mask\"])]).persist()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "86",
   "metadata": {},
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "raw",
   "id": "87",
   "metadata": {},
   "source": [
    "bbox = {'latitude': [50, 53], 'longitude': [1, 3]}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88",
   "metadata": {},
   "source": [
    "%%time\n",
    "# visualize states and emission matrix.  Save the visualisation in an html file.  \n",
    "# \n",
    "plot1 = visualization.plot_map(data[\"states\"],bbox)\n",
    "plot2 = visualization.plot_map(data[\"emission\"],bbox)\n",
    "plot=hv.Layout([plot1, plot2]).cols(2)\n",
    "filepath = f\"{target_root}/states_emission.html\"\n",
    "\n",
    "# save_html_hvplot(plot, filepath, storage_options)\n",
    "\n",
    "plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89",
   "metadata": {},
   "source": [
    "tag[\"tagging_events\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90",
   "metadata": {},
   "source": [
    "data[\"emission\"].isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91",
   "metadata": {},
   "source": [
    "%%time\n",
    "## Create Movies \n",
    "#\n",
    "mov = xmovie.Movie(\n",
    "    (data\n",
    "     .isel(time=slice(0,data.time.size-1,time_step))\n",
    "     .chunk({\"time\": 1, \"x\": -1, \"y\": -1})\n",
    "     .pipe(lambda ds: ds.merge(ds[[\"longitude\", \"latitude\"]].compute())))\n",
    "    .pipe(\n",
    "        visualization.filter_by_states\n",
    "    ),\n",
    "    plotfunc=visualization.create_frame,\n",
    "    input_check=False,\n",
    "    pixelwidth=15 * 400,\n",
    "    pixelheight=12 * 400,\n",
    "    dpi=400,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "92",
   "metadata": {},
   "source": [
    "if target_root.startswith('s3://'):\n",
    "    !mkdir -p movie\n",
    "    mov.save(f\"./movie/states.mp4\",  overwrite_existing=True, )\n",
    "    import s3fs\n",
    "    s3 = s3fs.S3FileSystem(**storage_options)\n",
    "    s3.put_file(f\"./movie/states.mp4\",f\"{target_root}/states.mp4\")\n",
    "else:\n",
    "    mov.save(f\"{target_root}/states.mp4\",  overwrite_existing=True, )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
