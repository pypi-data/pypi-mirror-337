[Observability Quick Start | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability): LLM should read this page when getting started with LangSmith observability, instrumenting an application to trace LLM calls, or tracing an entire application pipeline. Provides a step-by-step tutorial on installing dependencies, creating an API key, setting environment variables, defining a sample application, tracing just OpenAI calls or the entire application pipeline using decorators, and suggestions for next steps.

[Concepts | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/concepts): LLM should read this page when configuring an application to log traces to LangSmith, understanding the different data types and concepts used in LangSmith, or planning data storage and retention for traces. The page covers key concepts like Runs, Traces, Projects, Feedback, Tags, and Metadata, and explains data storage, retention, and deletion policies for traces in LangSmith.

[Observability how-to guides | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides): LLM should read this page when needing to add tracing to an LLM application, view and interact with traces, or configure alerts and automation rules. This page provides step-by-step guides on setting up tracing, integrating with various frameworks, configuring advanced tracing options, using the tracing UI, creating dashboards, setting up automation rules, and logging user feedback.

[Access the current run (span) within a traced function | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/access_current_span): LLM should read this page when trying to access the current run (span) within a traced function, when needing to extract UUIDs or tags from the current run, or when needing to access other information from the current run. This page explains how to access the current run (span) within a traced function using the get_current_run_tree/getCurrentRunTree function in the Python or TypeScript SDK, and provides examples for doing so.

[Add metadata and tags to traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/add_metadata_tags): LLM should read this page when adding metadata or tags to traces, understanding how to query traces by metadata/tags, or associating additional information with traces. The page explains how to add arbitrary metadata and tags to traces in Python and TypeScript, providing code examples for different methods like decorators, context managers, and wrapped clients.

[Annotate code for tracing | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/annotate_code): LLM should read this page when: 1) Learning how to log traces to LangSmith 2) Annotating code for tracing 3) Ensuring traces are submitted before application exit 'This page covers several approaches to annotate code for logging traces to LangSmith, including using decorators/wrappers, context managers, wrapping the OpenAI client, and using the RunTree API. It also discusses ensuring all traces are submitted before application exit.'

[Calculate token-based costs for traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/calculate_token_based_costs): LLM should read this page when calculating token-based costs for traces, setting up model pricing maps, or tracking costs for LLM invocations. This page explains how to set up a model pricing map in LangSmith and provide token counts for LLM runs to enable token-based cost calculation and tracking for traces.

[Compare traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/compare_traces): LLM should read this page when comparing traces in LangSmith, understanding the functionality of the trace comparison feature, and learning how to use the trace comparison view. This page explains how to access the trace comparison view in LangSmith, select traces for comparison, and navigate the side-by-side comparison of traces.

[Create dashboards | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/dashboards): LLM should read this page when creating custom dashboards for monitoring metrics, visualizing agent decisions at specific nodes, and comparing performance across models or configurations. This page covers creating and configuring dashboards with charts, applying filters, comparing metrics or data series within charts, managing charts, and using expanded chart views. It also provides a user journey example for visualizing agent decisions at a node.

[[Beta] Bulk Exporting Trace Data | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/data_export): LLM should read this page when needing to export trace data from LangSmith for offline analysis, needing to import trace data into other data analysis tools, or troubleshooting issues with data exports. This page covers how to bulk export trace data from LangSmith to an S3 bucket, how to monitor and manage export jobs, how to partition and import exported data into tools like BigQuery and Snowflake, and how to troubleshoot common errors during data exports.

[Implement distributed tracing | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/distributed_tracing): LLM should read this page when tracing requests across multiple services, implementing distributed tracing, or propagating trace context across boundaries. Covers setting up distributed tracing in Python and TypeScript, including examples for propagating trace context via headers and continuing traces across client-server boundaries.

[Query traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/export_traces): LLM should read this page when: 1) Querying traces based on specific conditions (e.g., run name, feedback score, latency, tokens, errors, timestamps, tags) 2) Analyzing trace data in advanced scenarios (e.g., flattened trace view, retriever I/O) 3) Exporting trace data for further analysis 'This page provides detailed guidance on querying LangSmith traces using filter arguments, query language syntax, and advanced techniques like exporting flattened trace views or retriever I/O. It covers various use cases for filtering traces based on run properties, metadata, feedback, latency, tokens, errors, timestamps, tags, and combining multiple conditions.'

[Filter traces in the application | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/filter_traces_in_application): LLM should read this page when filtering traces in the LangSmith application, constructing complex trace filters, and saving/managing custom trace filters. This page provides a comprehensive guide on how to filter traces in the LangSmith application, including creating filters for intermediate runs, filtering based on inputs/outputs or key-value pairs, negative filtering, saving and managing custom filters, copying filters, filtering within the trace view, manually specifying raw queries, and using AI-generated queries.

[Log custom LLM traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_llm_trace): LLM should read this page when logging traces for custom LLM models, using streaming output, or providing manual token counts. Provides guidelines for logging LLM traces in the expected format for chat and instruct-style models, handling streaming outputs, and manually providing token usage data.

[Log multimodal traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_multimodal_traces): LLM should read this page when logging multimodal traces with images, using LangSmith for tracing multimodal LLM runs, working with LangChain or other LLM libraries to log image data This page explains how to log images as part of multimodal traces in LangSmith, including code examples in Python and TypeScript for using the wrap_openai/wrapOpenAI functions to pass image URLs or base64 data to the OpenAI API. It shows how the images are rendered in the LangSmith UI trace.

[Log retriever traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_retriever_trace): LLM should read this page when logging retriever traces in an application, understanding how to format retriever data for rendering, understanding how retriever traces are displayed in LangSmith This page explains how to log and format retriever traces in LangSmith, including annotating the retriever step, returning a list of documents with specific keys, and how retriever traces are rendered.

[Log traces to specific project | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/log_traces_to_project): LLM should read this page when needing to log traces to a specific project, changing the destination project statically or dynamically. This page explains how to set the project name for traces through environment variables or at runtime, with code examples in Python and TypeScript.

[Prevent logging of sensitive data in traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/mask_inputs_outputs): LLM should read this page when: 1) Needing to prevent sensitive data from being logged in application traces 2) Requiring customization of input/output masking for specific functions Explains how to hide inputs and outputs entirely, use regex patterns or third-party services for rule-based masking, and process inputs/outputs for individual traced functions.

[Troubleshoot trace nesting | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/nest_traces): LLM should read this page when troubleshooting trace nesting issues, when encountering split or disconnected traces, or when working with asyncio, threading, or context propagation in Python. Provides guidance on resolving trace nesting problems caused by context propagation issues with asyncio and threading in Python, offering solutions like upgrading Python version, manual context propagation, and using LangSmith's ContextThreadPoolExecutor.

[Set up online evaluations | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/online_evaluations): LLM should read this page when setting up online evaluations, configuring LLM or custom code evaluators, and testing evaluation functions. This page explains how to configure online evaluations on LangSmith, including setting up LLM-based evaluators with prompts and custom code evaluators with provided guidelines and restrictions.

[How to print detailed logs (Python SDK) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/output_detailed_logs): LLM should read this page when needing to troubleshoot logging issues, adjusting log verbosity levels, or configuring logging for Python environments like Jupyter notebooks. Covers enabling Python's built-in logging mechanism, increasing logging verbosity for debugging, and configuring logging specifically for the LangSmith package.

[Set up automation rules | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/rules): LLM should read this page when setting up automation rules for LLM applications, viewing logs for automation rules, or understanding data retention This page explains how to create automation rules in LangSmith to trigger actions like online evaluations, adding traces to datasets, triggering webhooks, and extending data retention based on filters. It also covers viewing logs for automation rules.

[Set a sampling rate for traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/sample_traces): LLM should read this page when configuring sampling rate for traces, troubleshooting trace logging, optimizing performance. Explains how to set a sampling rate for traces logged to LangSmith, using the LANGSMITH_TRACING_SAMPLING_RATE environment variable to control the percentage of traces logged.

[Trace JS functions in serverless environments | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/serverless_environments): LLM should read this page when tracing JavaScript functions in serverless environments, ensuring tracing data is flushed before function completion, handling high concurrency scenarios. This page covers how to trace JavaScript functions in serverless environments using the LangSmith JS SDK, including setting environment variables, awaiting pending trace batches, handling rate limits at high concurrency, and manually flushing tracing data.

[Share or unshare a trace publicly | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/share_trace): LLM should read this page when sharing traces publicly, unsharing traces, or managing publicly shared traces. Covers how to share or unshare a trace publicly on LangSmith, including steps for sharing, unsharing from the trace view or list, and ensuring no sensitive data is exposed.

[Set up threads | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/threads): LLM should read this page when it needs to track multi-turn conversations or chatbot-like interactions, it needs to group related traces into threads, it needs to continue an existing conversation thread. This page explains how to associate traces together into threads using metadata keys like 'session_id' or 'thread_id', provides code examples for setting up threads in Python and TypeScript, and shows how to view conversation histories grouped into threads in the LangSmith UI.

[Toubleshooting variable caching | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/toubleshooting_variable_caching): LLM should read this page when troubleshooting issues with variable caching, when tracing data is not appearing as expected, or when traces are being logged to the wrong project/workspace. Provides steps to verify environment variables, clear the cache, and reload environment variables to resolve issues related to variable caching and tracing in LangSmith.

[Trace generator functions | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_generator_functions): LLM should read this page when tracing generator functions, streaming LLM outputs, or customizing output aggregation. Explains how to trace generator and async generator functions, stream outputs to minimize latency, and aggregate streamed outputs into desired formats before tracing them.

[Trace LangChain with OpenTelemetry | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_langchain_with_otel): LLM should read this page when integrating LangChain with OpenTelemetry for tracing, setting up distributed tracing with LangChain, or sending traces to alternate observability providers. The page explains how to enable OpenTelemetry integration in LangChain, create traced applications, view traces in LangSmith, set up distributed tracing with context propagation, configure alternate OTLP endpoints, and use the OpenTelemetry Collector for fan-out tracing.

[Trace using the LangSmith REST API | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_api): LLM should read this page when tracing LLM applications using the LangSmith REST API, ingesting multiple traces in batches, or serializing runs for multipart requests. This page provides guidance on tracing requests using the LangSmith REST API, including basic tracing, batch ingestion of traces, and serializing runs for multipart requests.

[Trace with Instructor (Python only) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_instructor): LLM should read this page when tracing Instructor Python code with LangSmith, using Instructor with OpenAI API, tracing nested Instructor function calls Provides guidance on setting up LangSmith tracing for Instructor, a Python library for generating structured outputs from LLMs, including wrapping OpenAI client, annotating functions with @traceable decorator

[Trace with LangChain (Python and JS/TS) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langchain): LLM should read this page when: 1) Integrating LangChain with LangSmith for tracing and observability 2) Logging traces with metadata, tags, and custom run/project names 3) Accessing run IDs and ensuring traces are submitted before exiting 'Guide for tracing LangChain with LangSmith for observability. Covers environment setup, logging traces, configuring metadata/tags/names, accessing run IDs, and distributed tracing. Also covers interoperability between LangChain and LangSmith SDK.'

[Trace with LangGraph (Python and JS/TS) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_langgraph): LLM should read this page when needing to trace LangChain and LangGraph workflows, needing to configure tracing for custom functions in those workflows Explains how to set up tracing for LangChain/LangGraph workflows using environment variables, tracing built-in LangChain modules, wrapping custom functions/SDKs with LangSmith decorators to trace

[Trace with OpenAI Agents SDK | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_openai_agents_sdk): LLM should read this page when integrating the OpenAI Agents SDK with LangSmith, tracing agent execution flows, or logging agent traces to LangSmith. This page provides instructions for installing the LangSmith library with OpenAI Agents support, and a code example demonstrating how to use the OpenAIAgentsTracingProcessor class to trace and log agent execution flows to LangSmith.

[Trace with OpenTelemetry | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_opentelemetry): LLM should read this page when tracing AI/LLM applications using OpenTelemetry, using the Traceloop SDK, or using the Arize SDK. This page covers how to set up and log traces from OpenTelemetry-based clients, the Traceloop SDK, and the Arize SDK to LangSmith for observability.

[Trace with the Vercel AI SDK (JS/TS only) | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_with_vercel_ai_sdk): LLM should read this page when needing to trace runs from the Vercel AI SDK, needing to customize tracing settings for Vercel AI SDK runs, or needing to set up tracing for different environments like Next.js, Node.js, Cloudflare Workers, etc. This page provides a guide on how to use LangSmith to trace runs from the Vercel AI SDK, including installation, environment configuration, logging traces, customizing run names and IDs, nesting runs, using a custom LangSmith client, debugging, adding metadata, and a deprecated method for older versions.

[Trace without setting environment variables | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/trace_without_env_vars): LLM should read this page when needing to trace code without setting environment variables, when needing to programmatically set tracing configuration. This page covers how to enable or disable tracing without setting environment variables, using the tracing_context context manager in Python and passing configuration to the traceable decorator in TypeScript.

[Upload files with traces | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/upload_files_with_traces): LLM should read this page when uploading files with traces, working with multimodal inputs or outputs, or tracing LLM pipelines that involve files. This page explains how to upload binary files like images, audio, videos, PDFs, and CSVs along with traces using the Python and TypeScript SDKs of LangSmith.

[Use monitoring charts | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/use_monitoring_charts): LLM should read this page when needing to analyze monitoring data for LLM applications, view data over different time periods, or drill down into specific subsets of traces. This page explains how to use monitoring charts in LangSmith to visualize metrics, change time periods, slice data by metadata/tags, and drill into specific subsets of traces.

[Set up webhook notifications for rules | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/how_to_guides/webhooks): LLM should read this page when setting up webhook notifications for automation rules, integrating with third-party services, or building custom event handling workflows. This page explains how to configure webhook URLs to receive notifications when LangSmith automation rules are triggered, including details on the webhook payload structure, security considerations, and an example implementation using Modal.

[Observability tutorials | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/tutorials): LLM should read this page when getting started with observability for LLM applications, learning about tracing LLM workflows, or exploring different tracing integrations. The page provides an overview of observability tutorials, including a quick start guide, in-depth tutorials on adding observability to LLM applications, and a list of how-to guides covering various observability-related topics.

[Add observability to your LLM application | 🦜️🛠️ LangSmith](https://docs.smith.langchain.com/observability/tutorials/observability): LLM should read this page when developing an LLM application, adding observability to an existing LLM application, or integrating LangSmith's observability features into their application. This page provides a tutorial on adding observability to an LLM application using LangSmith, covering prototyping, beta testing, and production stages.

