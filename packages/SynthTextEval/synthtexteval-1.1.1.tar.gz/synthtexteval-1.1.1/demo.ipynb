{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'princeton-nlp/Sheared-LLaMA-1.3B'\n",
    "dataset_name = 'tab'\n",
    "target_epsilon = 'inf'\n",
    "model_config = f'{model_name.replace(\"/\", \"_\")}_{dataset_name}_DP_{target_epsilon}'\n",
    "synthetic_data_path = f'./data/synthetic/{model_config}_outputs-final.csv'# Path to the CSV file where the outputs are saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downstream Utility Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import TrainingArguments as HfTrainingArguments\n",
    "from synthtexteval.eval.downstream.classify.train_classifier import TrainingArguments, ModelArguments, Classifier, Arguments\n",
    "from synthtexteval.utils.utils import create_classification_dataset\n",
    "from synthtexteval.utils.filtering import process_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: Creating the dataset\n",
    "\n",
    "Filtering data and creating a structured format out of raw synthetic text.\n",
    "\n",
    "We have assumed that the synthetic text is generated with labels (the labels typically serve as control codes in most setups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mapping from the original data\n",
    "# And creating a test set for evaluating the model once it is trained\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "from synthtexteval.utils.utils import encode_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data = load_from_disk('./data/generator/data/tab/')\n",
    "col_names = [i for i in tab_data['train'].column_names if i not in ['country', 'text', 'year']]\n",
    "tab_data = tab_data.remove_columns(col_names)\n",
    "tab_data['train'] = concatenate_datasets([tab_data['train'], tab_data['validation'], tab_data['test']])\n",
    "\n",
    "_, _ = encode_labels(tab_data['train'], label_column = 'country', json_mapping_exists = False, \n",
    "                                json_mapping_path = f'./data/benchmark/classification/data/{dataset_name}-mapping.json', multilabel=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab_data = load_from_disk('./data/generator/data/tab/')\n",
    "df, _ = encode_labels(tab_data['validation'], label_column = 'country', json_mapping_exists = True, \n",
    "                                json_mapping_path = f'./data/benchmark/classification/data/{dataset_name}-mapping.json', multilabel=False)\n",
    "test_file_path = f'./data/benchmark/classification/data/test/{dataset_name}/test.csv'\n",
    "print(f\"Saving test file to: {test_file_path}\")\n",
    "df.to_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this mapping for converting the synthetic data to a structured format\n",
    "df = pd.read_csv(synthetic_data_path)\n",
    "df = process_df(df, text_column = 'output_text')\n",
    "_, _, _ = create_classification_dataset(df, label_column = 'country', json_mapping_path = f'./data/benchmark/classification/data/{dataset_name}-mapping.json', json_mapping_exists = True,\n",
    "                                        output_dir = f'./data/benchmark/classification/data/{model_config}', multilabel = False, train_ratio = 0.7, test_ratio = 0.15, val_ratio = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/benchmark/classification/data/{dataset_name}-mapping.json') as f:\n",
    "    data = json.load(f)\n",
    "    n_labels_task = len(data)\n",
    "print(f\"Number of labels: {n_labels_task}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: Training the model\n",
    "\n",
    "This can also be run as a script. Sample script provided in eval.downstream.classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        train_args, model_args = TrainingArguments(), ModelArguments()\n",
    "\n",
    "        model_args.model_name = 'bert-base-uncased'\n",
    "        model_args.text_field = 'output_text'\n",
    "        model_args.label_field = 'Label'\n",
    "        model_args.path_to_dataset = f'./data/benchmark/classification/data/{model_config}'\n",
    "        model_args.path_to_model = f'./data/benchmark/classification/models/{model_args.model_name}_{model_config}'\n",
    "        model_args.n_labels = n_labels_task\n",
    "        model_args.is_train = True\n",
    "        model_args.problem_type = 'single_label_classification'\n",
    "        args = Arguments(train=train_args, model=model_args)\n",
    "\n",
    "        print(\"Training:\\n\")\n",
    "        obj = Classifier(args = args)\n",
    "        obj.finetune_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "        train_args, model_args = TrainingArguments(), ModelArguments()\n",
    "        model_args.is_train = False\n",
    "        model_args.is_test = True\n",
    "        model_args.text_field = 'text'\n",
    "        model_args.label_field = 'Label'\n",
    "\n",
    "        model_args.model_name = 'bert-base-uncased'\n",
    "        model_args.path_to_model = f'./data/benchmark/classification/models/{model_args.model_name}_{model_config}'\n",
    "        model_args.path_to_dataset = f'./data/benchmark/classification/data/test/{dataset_name}/test.csv'\n",
    "        model_args.path_to_output_csv = f'./data/benchmark/classification/test-results/{model_args.model_name}_{model_config}_test_outputs.csv'\n",
    "        model_args.path_to_aggregated_results = './data/benchmark/classification/compiled_benchmark_results.csv'\n",
    "\n",
    "        model_args.n_labels = n_labels_task\n",
    "        model_args.problem_type = \"single_label_classification\"\n",
    "        model_args.retain_columns = ['country', 'year']\n",
    "\n",
    "        args = Arguments(train=train_args, model=model_args)\n",
    "        print(\"Testing:\\n\")\n",
    "        obj = Classifier(args = args)\n",
    "        obj.test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification: Fairness auditing of the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthtexteval.eval.downstream.classify.visualize import tabulate_results\n",
    "\n",
    "path_to_test_output = f'./data/benchmark/classification/test-results/{model_args.model_name}_{model_config}_test_outputs.csv'\n",
    "tabulate_results([path_to_test_output], n_labels = n_labels_task, print_fairness=True, subgroup_type=\"country\", problem_type = \"multiclass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Analysis of Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from synthtexteval.eval.descriptive.descriptor import TextDescriptor\n",
    "from synthtexteval.eval.descriptive.arguments import TextDescriptorArgs\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "synth_df = pd.read_csv(synthetic_data_path)\n",
    "real_texts = load_from_disk('./data/generator/data/tab')\n",
    "len_samples = len(synth_df) if len(synth_df)<len(real_texts['train']) else len(real_texts['train'])\n",
    "synth_df = synth_df.head(len_samples)\n",
    "real_texts = real_texts['train'].select(range(len_samples))\n",
    "real_texts = real_texts[ 'text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text length and distributional comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_analyze = TextDescriptor(texts = synth_df['output_text'].tolist(), args = TextDescriptorArgs(produce_plot=True), reference_texts = real_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_analyze._compare_to_reference_distribution(metrics = ['text-length', 'jaccard', 'cosine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "tm = desc_analyze._topic_modeling_display(num_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy Leakage Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Privacy: Defining the entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from datasets import load_from_disk\n",
    "\n",
    "real_texts = load_from_disk('./data/generator/data/tab')\n",
    "real_texts = real_texts['train']\n",
    "synth_df = pd.read_csv(synthetic_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "for i in real_texts['annotations']:\n",
    "    try:\n",
    "        for annotator in i:\n",
    "            for entity in i[annotator]['entity_mentions']:\n",
    "                if(entity['entity_type'] in ['PERSON', 'DATETIME']):\n",
    "                    entities.append(entity['span_text'])\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(entities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating leakage of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthtexteval.eval.privacy.metrics import entity_leakage, search_and_compute_EPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_leakage, privacy_analysis = entity_leakage(synth_df['output_text'].tolist(), entities, 'privacy-leakage.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of leaked entities: {100*total_leakage:.3f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating span memorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing this only for 15 entities as it is time-intensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = entities[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_df = pd.DataFrame({'text': synth_df['output_text'].tolist()[:10]})\n",
    "\n",
    "\n",
    "search_and_compute_EPO(synth_file = synth_df, ref_file = t_df, \n",
    "                       synth_phrase_file_path = 'synth-outputs.csv', ref_phrase_file_path = 'ref-outputs.csv',\n",
    "                       entity_patterns = fake_entities, max_window_len = 3,\n",
    "                       text_field = text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percentage of leaked entity contexts: {100*total_leakage:.3f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Evaluation Against Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from synthtexteval.eval.text_quality.metrics import TextQualityEval\n",
    "from synthtexteval.eval.text_quality.arguments import MauveArgs, LMArgs, FrechetArgs\n",
    "from dataclasses import dataclass\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "df = pd.DataFrame({})\n",
    "synthetic_samples = pd.read_csv(synthetic_data_path)\n",
    "real_samples = load_from_disk('./data/generator/data/tab')\n",
    "len_samples = len(synthetic_samples) if len(synthetic_samples)<len(real_samples['train']) else len(real_samples['train'])\n",
    "synthetic_samples = synthetic_samples.head(len_samples)\n",
    "real_samples = real_samples['train'].select(range(len_samples))\n",
    "\n",
    "df['source'] = synthetic_samples['output_text']\n",
    "df['reference'] = real_samples['text']\n",
    "\n",
    "@dataclass\n",
    "class args_temp:\n",
    "    FrechetArgs:FrechetArgs\n",
    "    MauveArgs:MauveArgs\n",
    "    LMArgs:LMArgs\n",
    "\n",
    "args_ = args_temp(FrechetArgs, MauveArgs, LMArgs)\n",
    "qual_estimator = TextQualityEval(args_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_estimator.calculate_perplexity(df)\n",
    "qual_estimator.calculate_fid_score(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_estimator.print_metrics(qual_estimator.return_results())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
