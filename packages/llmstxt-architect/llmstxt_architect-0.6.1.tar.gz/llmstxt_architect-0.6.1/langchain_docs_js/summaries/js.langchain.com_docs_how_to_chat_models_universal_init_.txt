[](https://js.langchain.com/docs/how_to/chat_models_universal_init/): LLM should read this page when: 1) Initializing and configuring different chat models in a LLM application 2) Allowing end users to specify the model provider and model to use 3) Declaratively binding tools, configuring structured output, etc. with a configurable model The page covers how to use the initChatModel() helper to initialize different chat model integrations from OpenAI, Anthropic, Google, etc. It shows how to create configurable models that can be initialized with different models/providers at runtime, including specifying configurable fields and default values. It also demonstrates using configurable models declaratively with tool binding, structured output, etc.

